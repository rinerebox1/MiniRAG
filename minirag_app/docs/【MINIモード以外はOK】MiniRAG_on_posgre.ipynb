{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6875c5de-f21a-4610-a8fc-ef219ee61dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要なライブラリのインポート\n",
    "import os\n",
    "import tempfile\n",
    "from minirag import MiniRAG, QueryParam\n",
    "from minirag.llm.hf import (\n",
    "    hf_model_complete,\n",
    "    hf_embed,\n",
    ")\n",
    "# from minirag.llm.openai import openrouter_openai_complete\n",
    "from minirag.llm.openai import openai_complete_if_cache\n",
    "from minirag.utils import EmbeddingFunc\n",
    "from minirag.utils import (\n",
    "    wrap_embedding_func_with_attrs,\n",
    "    locate_json_string_body_from_string,\n",
    "    safe_unicode_decode,\n",
    "    logger,\n",
    ")\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from tenacity import retry, stop_after_attempt, wait_random_exponential, retry_if_exception\n",
    "from openai import RateLimitError, APIStatusError # openaiライブラリが投げる例外をインポート\n",
    "import asyncio\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a8f379-c151-472f-9720-3d9e063628fe",
   "metadata": {},
   "source": [
    "### データベース接続テスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "913d86fc-9456-4a42-baf5-cbcf7ec23318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'version': 'PostgreSQL 16.9 (Debian 16.9-1.pgdg120+1) on x86_64-pc-linux-gnu, compiled by gcc (Debian 12.2.0-14) 12.2.0, 64-bit'}\n"
     ]
    }
   ],
   "source": [
    "import psycopg\n",
    "from psycopg.rows import dict_row\n",
    "\n",
    "def get_conn():\n",
    "    url = os.getenv(\"DATABASE_URL\")\n",
    "    if url:\n",
    "        return psycopg.connect(url, row_factory=dict_row)\n",
    "    # fallback: assemble from separate vars\n",
    "    dsn = (\n",
    "        f\"host={os.getenv('PGHOST','db')} \"\n",
    "        f\"port={os.getenv('PGPORT','5432')} \"\n",
    "        f\"dbname={os.getenv('PGDATABASE','postgres')} \"\n",
    "        f\"user={os.getenv('POSTGRES_USER','postgres')} \"\n",
    "        f\"password={os.getenv('POSTGRES_PASSWORD')}\"\n",
    "    )\n",
    "    return psycopg.connect(dsn, row_factory=dict_row)\n",
    "\n",
    "with get_conn() as conn, conn.cursor() as cur:\n",
    "    cur.execute(\"SELECT version();\")\n",
    "    print(cur.fetchone())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f907bca4-e72e-4308-a6f3-2872d0324002",
   "metadata": {},
   "source": [
    "### ハイブリッドクエリのテスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9a51793-190e-4933-b145-adbc7c90679e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "データベースに接続し、クエリを実行します...\n",
      "\n",
      "--- クエリ実行結果 ---\n",
      "{'id': 1, 'name': '商品A', 'embedding': '[0.1,0.2,0.3]'}\n",
      "{'id': 2, 'name': '商品B', 'embedding': '[0.2,0.1,0.9]'}\n"
     ]
    }
   ],
   "source": [
    "def get_conn():\n",
    "    \"\"\"\n",
    "    環境変数からデータベース接続情報を取得し、接続オブジェクトを返す。\n",
    "    DATABASE_URLが設定されていればそれを使用し、なければ個別の変数を組み立てる。\n",
    "    結果は辞書形式で返されるように設定済み。\n",
    "    \"\"\"\n",
    "    url = os.getenv(\"DATABASE_URL\")\n",
    "    if url:\n",
    "        # DATABASE_URLが設定されている場合\n",
    "        return psycopg.connect(url, row_factory=dict_row)\n",
    "    \n",
    "    # DATABASE_URLがない場合、個別の環境変数からDSNを組み立てる\n",
    "    # PGPASSWORDが設定されていないと接続に失敗するため、チェックを追加するとより親切\n",
    "    password = os.getenv('PGPASSWORD')\n",
    "    if not password:\n",
    "        raise ValueError(\"環境変数 PGPASSWORD が設定されていません。\")\n",
    "        \n",
    "    dsn = (\n",
    "        f\"host={os.getenv('PGHOST', 'localhost')} \"\n",
    "        f\"port={os.getenv('PGPORT', '5432')} \"\n",
    "        f\"dbname={os.getenv('PGDATABASE', 'postgres')} \"\n",
    "        f\"user={os.getenv('POSTGRES_USER', 'postgres')} \"\n",
    "        f\"password={password}\"\n",
    "    )\n",
    "    return psycopg.connect(dsn, row_factory=dict_row)\n",
    "\n",
    "def find_recommended_products_for_alice(_results):\n",
    "    \"\"\"\n",
    "    指定されたSQLクエリを実行し、'Alice'が好む商品とベクトル的に類似した商品を取得する。\n",
    "    \"\"\"\n",
    "    # 実行したいSQLクエリを三重クォートで定義\n",
    "    # これにより、複数行のクエリを読みやすく記述できる\n",
    "    sql_query = \"\"\"\n",
    "    SELECT\n",
    "        p.id,\n",
    "        p.name,\n",
    "        p.embedding\n",
    "    FROM\n",
    "        public.products AS p\n",
    "    JOIN\n",
    "        -- cypher関数でグラフクエリを実行し、結果をテーブルのように扱う\n",
    "        cypher('my_minirag_graph', $$\n",
    "            MATCH (u:User {name: 'Alice'})-[:LIKES]->(prod:Product)\n",
    "            RETURN prod.product_id\n",
    "        $$) AS liked(product_id agtype)\n",
    "    ON\n",
    "        -- グラフクエリの結果(agtype)を整数にキャストしてproductsテーブルのIDと結合\n",
    "        p.id = (liked.product_id)::INTEGER\n",
    "    ORDER BY\n",
    "        -- pgvectorの<->演算子で、指定ベクトルとのコサイン距離が近い順に並び替え\n",
    "        p.embedding <=> '[0.1, 0.1, 0.2]';\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # with文で接続とカーソルを管理し、処理終了後に自動でクローズする\n",
    "        with get_conn() as conn, conn.cursor() as cur:\n",
    "            print(\"データベースに接続し、クエリを実行します...\")\n",
    "            \n",
    "            # クエリの実行\n",
    "            cur.execute(sql_query)\n",
    "            \n",
    "            # 全ての実行結果を取得 (fetchall)\n",
    "            # 1件だけなら fetchone(), 複数件なら fetchall() を使う\n",
    "            results = cur.fetchall()\n",
    "            \n",
    "            print(\"\\n--- クエリ実行結果 ---\")\n",
    "            if results:\n",
    "                # 取得した結果を1行ずつ表示\n",
    "                for row in results:\n",
    "                    print(row)\n",
    "                    _results.append(row)\n",
    "            else:\n",
    "                print(\"条件に一致する結果は見つかりませんでした。\")\n",
    "        return _results\n",
    "\n",
    "    except psycopg.Error as e:\n",
    "        print(f\"データベースエラーが発生しました: {e}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"設定エラーが発生しました: {e}\")\n",
    "\n",
    "query_results = []\n",
    "query_results = find_recommended_products_for_alice(query_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211ca750-bd82-43c8-9d52-421afb4771ed",
   "metadata": {},
   "source": [
    "### マークダウンに整形"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b391635-b718-4889-a334-52ab7e15fb45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| id | name | embedding |\n",
      "| --- | --- | --- |\n",
      "| 1 | 商品A | [0.1,0.2,0.3] |\n",
      "| 2 | 商品B | [0.2,0.1,0.9] |\n"
     ]
    }
   ],
   "source": [
    "def convert_to_markdown(data: list[dict]) -> str:\n",
    "    \"\"\"\n",
    "    辞書のリストをマークダウンのテーブル形式に変換します。\n",
    "\n",
    "    Args:\n",
    "        data: 辞書のリスト。各辞書がテーブルの1行に対応します。\n",
    "\n",
    "    Returns:\n",
    "        マークダウン形式のテーブル文字列。\n",
    "    \"\"\"\n",
    "    if not data:\n",
    "        return \"データがありません。\"\n",
    "\n",
    "    # ヘッダーの作成 (最初のデータのキーから取得)\n",
    "    headers = data[0].keys()\n",
    "    header_line = \"| \" + \" | \".join(headers) + \" |\"\n",
    "\n",
    "    # 区切り線の作成\n",
    "    separator_line = \"| \" + \" | \".join([\"---\"] * len(headers)) + \" |\"\n",
    "\n",
    "    # データ行の作成\n",
    "    data_lines = []\n",
    "    for row in data:\n",
    "        # 各値を文字列に変換して結合\n",
    "        values = [str(row.get(h, \"\")) for h in headers]\n",
    "        data_lines.append(\"| \" + \" | \".join(values) + \" |\")\n",
    "\n",
    "    # 全ての行を結合して返す\n",
    "    return \"\\n\".join([header_line, separator_line] + data_lines)\n",
    "\n",
    "# --- 変換を実行して結果を表示 ---\n",
    "markdown_table = convert_to_markdown(query_results)\n",
    "print(markdown_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ece4d57a-b30d-4275-b0ca-9a8e5435e645",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  id | name  |   embedding   \n",
    "# ----+-------+---------------\n",
    "#   1 | 商品A | [0.1,0.2,0.3]\n",
    "#   2 | 商品B | [0.2,0.1,0.9]\n",
    "\n",
    "# 上記の結果になればOK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d941081d-405c-4833-86fa-f5bcdd0b3c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Openrouter APIキーが設定されました\n",
      "\n",
      "HuggingFace TOKEN が設定されました\n"
     ]
    }
   ],
   "source": [
    "# API KEY が見えないようにコメントアウト\n",
    "\n",
    "# print(os.getenv(\"OPENROUTER_API_KEY\"))\n",
    "# print(os.getenv(\"OPENAI_API_KEY\"))\n",
    "print(\"Openrouter APIキーが設定されました\")\n",
    "print()\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "# print(HF_TOKEN)\n",
    "print(\"HuggingFace TOKEN が設定されました\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c16a765-6d23-439b-a433-e40df1cb977d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "作業ディレクトリ: /tmp/minirag_demo\n"
     ]
    }
   ],
   "source": [
    "# 埋め込みモデルの設定\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "EMBEDDING_DIM = 384\n",
    "# MINI モードでの回答ができなくなったので、逆に精度が落ちたかも。\n",
    "# EMBEDDING_MODEL = \"hotchpotch/static-embedding-japanese\"\n",
    "# EMBEDDING_DIM = 1024\n",
    "\n",
    "TOKENIZER_MODEL = \"hotchpotch/xlm-roberta-japanese-tokenizer\"\n",
    "\n",
    "# LLMの設定\n",
    "# LLM_MODEL = \"Qwen/Qwen3-1.7B\"  # または \"Qwen/Qwen3-4B\", \"Qwen/Qwen3-1.7B\" など\n",
    "# LLM_MODEL = \"jaeyong2/Qwen2.5-3B-Instruct-Ja-SFT\"\n",
    "LLM_MODEL = \"deepseek/deepseek-chat-v3-0324:free\"\n",
    "\n",
    "\n",
    "\n",
    "# 作業ディレクトリの作成\n",
    "WORKING_DIR = \"/tmp/minirag_demo\"\n",
    "os.makedirs(WORKING_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"作業ディレクトリ: {WORKING_DIR}\")\n",
    "\n",
    "\n",
    "# DATA_PATH = args.datapath\n",
    "# QUERY_PATH = args.querypath\n",
    "# OUTPUT_PATH = args.outputpath\n",
    "# print(\"USING LLM:\", LLM_MODEL)\n",
    "# print(\"USING WORKING DIR:\", WORKING_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30f93e21-c206-4407-8f94-8e670dc30fd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0259b8307f349bfae169dd2e77693d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbcfc3b085fd4598af5dbb91e9378810",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f75d769358f43a49fada70a6bee5353",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93b094718a5e4870bad8606a1393e24a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b10270937981496a8312b6c79ed5d159",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06e343abbd0841e38690d2b01a1fd0fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca10a2c67bc84e7896519962dccbd9aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b61a7724b5f94da399c90f38b9bf4120",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56d1a147c6b0479dad71c145000563b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd58e6c6a5fd4c82bcda3c28c41b1bef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f2af5b7deae40a6bf9774268cc4ae69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 384)\n",
      "0.5309: 素敵なカフェが近所にあるよ。落ち着いた雰囲気でゆっくりできるし、窓際の席からは公園の景色も見えるんだ。\n",
      "0.4410: 新鮮な魚介を提供する店です。地元の漁師から直接仕入れているので鮮度は抜群ですし、料理人の腕も確かです。\n",
      "0.5744: あそこは行きにくいけど、隠れた豚骨の名店だよ。スープが最高だし、麺の硬さも好み。\n",
      "0.5348: おすすめの中華そばの店を教えてあげる。とりわけチャーシューが手作りで柔らかくてジューシーなんだ。\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(EMBEDDING_MODEL, device=\"cpu\", token=HF_TOKEN)\n",
    "\n",
    "query = \"美味しいラーメン屋に行きたい\"\n",
    "docs = [\n",
    "    \"素敵なカフェが近所にあるよ。落ち着いた雰囲気でゆっくりできるし、窓際の席からは公園の景色も見えるんだ。\",\n",
    "    \"新鮮な魚介を提供する店です。地元の漁師から直接仕入れているので鮮度は抜群ですし、料理人の腕も確かです。\",\n",
    "    \"あそこは行きにくいけど、隠れた豚骨の名店だよ。スープが最高だし、麺の硬さも好み。\",\n",
    "    \"おすすめの中華そばの店を教えてあげる。とりわけチャーシューが手作りで柔らかくてジューシーなんだ。\",\n",
    "]\n",
    "\n",
    "embeddings = model.encode([query] + docs)\n",
    "print(embeddings.shape)\n",
    "similarities = model.similarity(embeddings[0], embeddings[1:])\n",
    "for i, similarity in enumerate(similarities[0].tolist()):\n",
    "    print(f\"{similarity:.04f}: {docs[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00688add-fcf5-4517-85c1-2eae3ae5f696",
   "metadata": {},
   "source": [
    "## transformer の API で使えるように変換する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87149d98-7010-4b7d-96c1-b0b2566fa72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import PreTrainedModel, PretrainedConfig\n",
    "from transformers.modeling_outputs import BaseModelOutputWithPoolingAndCrossAttentions\n",
    "\n",
    "\n",
    "class StaticEmbeddingConfig(PretrainedConfig):\n",
    "    model_type = \"static-embedding\"\n",
    "\n",
    "    def __init__(self, vocab_size=32768, hidden_size=1024, pad_token_id=0, **kwargs):\n",
    "        super().__init__(pad_token_id=pad_token_id, **kwargs)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "\n",
    "class StaticEmbeddingModel(PreTrainedModel):\n",
    "    config_class = StaticEmbeddingConfig\n",
    "\n",
    "    def __init__(self, config: StaticEmbeddingConfig):\n",
    "        super().__init__(config)\n",
    "        # ★ EmbeddingBag そのものでも OK ですが、\n",
    "        #   シーケンス長をそろえて attention_mask で平均を取る方が扱いやすいので nn.Embedding に変更\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=config.vocab_size,\n",
    "            embedding_dim=config.hidden_size,\n",
    "            padding_idx=config.pad_token_id,\n",
    "        )\n",
    "        self.post_init()  # transformers の重み初期化\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, **kwargs):\n",
    "        \"\"\"\n",
    "        - input_ids      : (batch, seq_len)\n",
    "        - attention_mask : (batch, seq_len) — 0 は padding\n",
    "        戻り値は Transformers 共通の BaseModelOutputWithPoolingAndCrossAttentions\n",
    "        \"\"\"\n",
    "        if attention_mask is None:\n",
    "            attention_mask = (input_ids != self.config.pad_token_id).int()\n",
    "\n",
    "        token_embs = self.embedding(input_ids)                       # (B, L, H)\n",
    "        # マスク付き平均プール\n",
    "        masked_embs = token_embs * attention_mask.unsqueeze(-1)      # (B, L, H)\n",
    "        lengths = attention_mask.sum(dim=1, keepdim=True).clamp(min=1e-8)  # (B, 1)\n",
    "        sentence_emb = masked_embs.sum(dim=1) / lengths              # (B, H)\n",
    "\n",
    "        return BaseModelOutputWithPoolingAndCrossAttentions(\n",
    "            last_hidden_state=token_embs,  # ここでは token レベルをそのまま\n",
    "            pooler_output=sentence_emb,    # 文ベクトル\n",
    "            attentions=None,\n",
    "            cross_attentions=None,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17442fc4-fc20-4821-8812-596ff97f4496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b619353366cf449d8cf4671b1c846cf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/134 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51a094df160c41fe88c186a7686b0ed4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/205 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b88213766a464cd28f8ec538028370f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "248d17155e264b9f8046887795d32170",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4a0dcdbdff74c6a8012485c82d5e85e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/134M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 変換完了 — 保存先: ./static-embedding-transformers\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SentenceTransformer 版 (hotchpotch/static-embedding-japanese) から\n",
    "StaticEmbeddingModel へ重みをコピーして保存するスクリプト\n",
    "\"\"\"\n",
    "\n",
    "SRC = \"hotchpotch/static-embedding-japanese\"   # オリジナル\n",
    "DST = \"./static-embedding-transformers\"        # 保存先\n",
    "\n",
    "# ① SentenceTransformer を読み込む\n",
    "st_model = SentenceTransformer(SRC)\n",
    "embedding_weight = st_model[0].embedding.weight.data   # nn.EmbeddingBag の重みを取得\n",
    "\n",
    "# ② Config → Model を作成\n",
    "config = StaticEmbeddingConfig(\n",
    "    vocab_size=embedding_weight.size(0),\n",
    "    hidden_size=embedding_weight.size(1),\n",
    "    pad_token_id=0,           # トークナイザの <pad> が id=0\n",
    ")\n",
    "model = StaticEmbeddingModel(config)\n",
    "\n",
    "# ③ 重みコピー\n",
    "with torch.no_grad():\n",
    "    model.embedding.weight.copy_(embedding_weight)\n",
    "\n",
    "# ④ save_pretrained で書き出し\n",
    "model.save_pretrained(DST)\n",
    "# st_model.tokenizer.save_pretrained(DST)   # tokenizer.json なども一緒に保存\n",
    "\n",
    "print(f\"✅ 変換完了 — 保存先: {DST}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd90d1ce-899e-4e4b-8c4b-ba6bc9545e41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f11572bcc05a460bbb8079c85e1f3f7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da36000738cc4ea784d81c23ca4dad18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05106dd2fd184b4886ae8e45f30b7d22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: torch.Size([2, 1024])\n",
      "cosine: 0.4834601879119873\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "MODEL_DIR = \"./static-embedding-transformers\"\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_MODEL)\n",
    "model     = StaticEmbeddingModel.from_pretrained(MODEL_DIR)\n",
    "\n",
    "sentences = [\n",
    "    \"美味しいラーメン屋に行きたい\",\n",
    "    \"あそこは行きにくいけど、隠れた豚骨の名店だよ。スープが最高だし、麺の硬さも好み。\",\n",
    "]\n",
    "\n",
    "inputs = tokenizer(\n",
    "    sentences,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    add_special_tokens=False,   # 元モデルは special tokens なし\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    vecs = outputs.pooler_output     # (batch, hidden_size)\n",
    "\n",
    "print(\"shape:\", vecs.shape)          # torch.Size([2, 1024])\n",
    "similarity = torch.nn.functional.cosine_similarity(vecs[0], vecs[1], dim=0)\n",
    "print(\"cosine:\", similarity.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5e501a8-78d3-431e-8849-cc7865683db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def openrouter_openai_complete(\n",
    "    prompt,\n",
    "    system_prompt=None,\n",
    "    history_messages=[],\n",
    "    keyword_extraction=False,\n",
    "    api_key: str = None,\n",
    "    **kwargs,\n",
    ") -> str:\n",
    "    # if api_key:\n",
    "    #     os.environ[\"OPENROUTER_API_KEY\"] = api_key\n",
    "\n",
    "    keyword_extraction = kwargs.pop(\"keyword_extraction\", None)\n",
    "    result = await openai_complete_if_cache(\n",
    "        LLM_MODEL,  # change accordingly\n",
    "        prompt,\n",
    "        system_prompt=system_prompt,\n",
    "        history_messages=history_messages,\n",
    "        base_url=\"https://openrouter.ai/api/v1\",\n",
    "        api_key=api_key,\n",
    "        **kwargs,\n",
    "    )\n",
    "    if keyword_extraction:  # TODO: use JSON API\n",
    "        return locate_json_string_body_from_string(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ea62583-fcbc-4e2a-9513-cc6bd4860f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/app\n",
      "MiniRAG_on_posgre.ipynb  minirag  setup.py  static-embedding-transformers\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31b8ac6d-2322-4339-ae19-9b5c6193e521",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nano-vectordb:Init {'embedding_dim': 384, 'metric': 'cosine', 'storage_file': '/tmp/minirag_demo/vdb_entities.json'} 0 data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 384, 'metric': 'cosine', 'storage_file': '/tmp/minirag_demo/vdb_entities_name.json'} 0 data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 384, 'metric': 'cosine', 'storage_file': '/tmp/minirag_demo/vdb_relationships.json'} 0 data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 384, 'metric': 'cosine', 'storage_file': '/tmp/minirag_demo/vdb_chunks.json'} 0 data\n",
      "INFO:minirag:Loaded document status storage with 0 records\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MiniRAGが初期化されました！\n"
     ]
    }
   ],
   "source": [
    "# MiniRAGインスタンスの作成\n",
    "rag = MiniRAG(\n",
    "    working_dir=WORKING_DIR,\n",
    "\n",
    "    # ポスグレ\n",
    "    # kv_storage=\"PGKVStorage\",\n",
    "    # vector_storage=\"PGVectorStorage\",\n",
    "    # graph_storage=\"PGGraphStorage\",\n",
    "\n",
    "    # llm_model_func=hf_model_complete,\n",
    "    # llm_model_func=gemini_2_5_flash_complete,\n",
    "    llm_model_func=openrouter_openai_complete,\n",
    "\n",
    "    llm_model_max_token_size=200,\n",
    "    llm_model_name=LLM_MODEL,\n",
    "    embedding_func=EmbeddingFunc(\n",
    "        embedding_dim=EMBEDDING_DIM,\n",
    "        max_token_size=1000,\n",
    "        func=lambda texts: hf_embed(\n",
    "            texts,\n",
    "            # tokenizer=tokenizer,\n",
    "            # embed_model=model,\n",
    "            tokenizer=AutoTokenizer.from_pretrained(EMBEDDING_MODEL),\n",
    "            embed_model=AutoModel.from_pretrained(EMBEDDING_MODEL),\n",
    "        ),\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"MiniRAGが初期化されました！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac44cacb-928d-4b21-acad-3ee53633b357",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:minirag:Stored 1 new unique documents\n",
      "INFO:minirag:Number of batches to process: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "データを挿入中...\n",
      "テキスト 1/4 を挿入中...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:minirag:Inserting 1 vectors to chunks\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:00<00:00,  1.51batch/s]\n",
      "INFO:minirag:Document processing pipeline completed\n",
      "INFO:minirag:Performing entity extraction on newly processed chunks\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 500 Internal Server Error\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.396025 seconds\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠙ Processed 1 chunks, 7 entities(duplicated), 4 relations(duplicated)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:minirag:Inserting 7 vectors to entities\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings:   0%|          | 0/1 [00:00<?, ?batch/s]We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:00<00:00,  1.47batch/s]\n",
      "INFO:minirag:Inserting 7 vectors to entities\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:00<00:00,  1.52batch/s]\n",
      "INFO:minirag:Inserting 7 vectors to entities_name\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:00<00:00,  1.46batch/s]\n",
      "INFO:minirag:Inserting 4 vectors to relationships\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:00<00:00,  1.49batch/s]\n",
      "INFO:minirag:Writing graph with 7 nodes, 4 edges\n",
      "INFO:minirag:Stored 1 new unique documents\n",
      "INFO:minirag:Number of batches to process: 1\n",
      "INFO:minirag:Inserting 1 vectors to chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "テキスト 2/4 を挿入中...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 1/1 [00:00<00:00,  1.56batch/s]\n",
      "INFO:minirag:Document processing pipeline completed\n",
      "INFO:minirag:Performing entity extraction on newly processed chunks\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠹ Processed 2 chunks, 17 entities(duplicated), 13 relations(duplicated)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:minirag:Inserting 17 vectors to entities\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 1/1 [00:01<00:00,  1.02s/batch]\n",
      "INFO:minirag:Inserting 17 vectors to entities\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:00<00:00,  1.33batch/s]\n",
      "INFO:minirag:Inserting 17 vectors to entities_name\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:00<00:00,  1.52batch/s]\n",
      "INFO:minirag:Inserting 13 vectors to relationships\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:00<00:00,  1.35batch/s]\n",
      "INFO:minirag:Writing graph with 19 nodes, 14 edges\n",
      "INFO:minirag:Stored 1 new unique documents\n",
      "INFO:minirag:Number of batches to process: 1\n",
      "INFO:minirag:Inserting 1 vectors to chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "テキスト 3/4 を挿入中...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 1/1 [00:00<00:00,  1.58batch/s]\n",
      "INFO:minirag:Document processing pipeline completed\n",
      "INFO:minirag:Performing entity extraction on newly processed chunks\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠙ Processed 1 chunks, 4 entities(duplicated), 3 relations(duplicated)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠸ Processed 3 chunks, 22 entities(duplicated), 16 relations(duplicated)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:minirag:Inserting 22 vectors to entities\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 1/1 [00:00<00:00,  1.17batch/s]\n",
      "INFO:minirag:Inserting 22 vectors to entities\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:00<00:00,  1.13batch/s]\n",
      "INFO:minirag:Inserting 22 vectors to entities_name\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:00<00:00,  1.56batch/s]\n",
      "INFO:minirag:Inserting 16 vectors to relationships\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:00<00:00,  1.37batch/s]\n",
      "INFO:minirag:Writing graph with 31 nodes, 27 edges\n",
      "INFO:minirag:Stored 1 new unique documents\n",
      "INFO:minirag:Number of batches to process: 1\n",
      "INFO:minirag:Inserting 1 vectors to chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "テキスト 4/4 を挿入中...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 1/1 [00:00<00:00,  1.58batch/s]\n",
      "INFO:minirag:Document processing pipeline completed\n",
      "INFO:minirag:Performing entity extraction on newly processed chunks\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 500 Internal Server Error\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.482531 seconds\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 500 Internal Server Error\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.380949 seconds\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠙ Processed 1 chunks, 0 entities(duplicated), 0 relations(duplicated)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠼ Processed 4 chunks, 16 entities(duplicated), 12 relations(duplicated)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:minirag:Inserting 16 vectors to entities\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 1/1 [00:00<00:00,  1.29batch/s]\n",
      "INFO:minirag:Inserting 16 vectors to entities\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:01<00:00,  1.07s/batch]\n",
      "INFO:minirag:Inserting 16 vectors to entities_name\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:01<00:00,  1.29s/batch]\n",
      "INFO:minirag:Inserting 12 vectors to relationships\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:01<00:00,  1.33s/batch]\n",
      "INFO:minirag:Writing graph with 38 nodes, 33 edges\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "すべてのデータが挿入されました！\n",
      "処理時間: 268.2555秒\n"
     ]
    }
   ],
   "source": [
    "# 約12分かかった\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# サンプルテキストデータ\n",
    "sample_texts = [\n",
    "    \"\"\"\n",
    "今日は素晴らしい一日でした。朝早く起きて、近所の公園を散歩しました。\n",
    "桜の花が満開で、とても美しかったです。午後は友人と映画を見に行きました。\n",
    "「君の名は。」という映画で、とても感動的でした。\n",
    "夜は家族と一緒に夕食を取り、楽しい時間を過ごしました。\n",
    "\"\"\",\n",
    "    \"\"\"\n",
    "昨日は仕事で大きなプロジェクトが完了しました。\n",
    "チーム全員で3ヶ月間取り組んできたAIシステムの開発が終わりました。\n",
    "機械学習モデルの精度が95%を超え、クライアントからも高い評価をいただきました。\n",
    "今夜はチームメンバーと祝賀会を開く予定です。\n",
    "\"\"\",\n",
    "    \"\"\"\n",
    "週末は料理に挑戦しました。初めてパスタを一から作ってみました。\n",
    "小麦粉から麺を作るのは思っていたより難しかったですが、\n",
    "最終的にはとても美味しいカルボナーラができました。\n",
    "次回はリゾットに挑戦してみたいと思います。\n",
    "\"\"\",\n",
    "    \"\"\"\n",
    "読書が趣味で、最近は村上春樹の「ノルウェイの森」を読んでいます。\n",
    "主人公の心情描写がとても繊細で、引き込まれます。\n",
    "また、技術書も読んでおり、「深層学習」について学んでいます。\n",
    "理論と実践のバランスが取れた良い本だと思います。\n",
    "\"\"\"\n",
    "]\n",
    "\n",
    "# データの挿入\n",
    "print(\"データを挿入中...\")\n",
    "\n",
    "async def insert_texts(rag_instance, texts):\n",
    "    for i, text in enumerate(texts):\n",
    "        print(f\"テキスト {i+1}/{len(texts)} を挿入中...\")\n",
    "        await rag_instance.ainsert(text.strip())\n",
    "\n",
    "    print(\"\\nすべてのデータが挿入されました！\")\n",
    "\n",
    "\n",
    "# イベントループが既に実行中の場合\n",
    "try:\n",
    "    await insert_texts(rag, sample_texts)\n",
    "except RuntimeError:\n",
    "    # 新しいループで実行\n",
    "    asyncio.run(insert_texts(rag, sample_texts))\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"処理時間: {elapsed_time:.4f}秒\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "35031a37-d7b7-4a17-875e-a3f165cdb288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "クエリ: 映画について教えて\n",
      "==================================================\n",
      "\n",
      "--- NAIVEモード ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:minirag:Query: 映画について教えて, top_k: 60, cosine_better_than_threshold: 0.4\n",
      "INFO:minirag:Truncate 1 to 1 chunks\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "回答: 申し訳ありませんが、提供いただいたテキストには映画に関する情報は含まれていません。現在のテキストでは週末の料理体験についてのみ記載されており、具体的には：\n",
      "\n",
      "- 初めてパスタを一から作ったこと（小麦粉から麺を作る工程の難しさ）\n",
      "- 完成したカルボナーラの美味しさ\n",
      "- 次回はリゾット作りに挑戦したいという意向\n",
      "\n",
      "映画についてお知りになりたいのであれば、具体的な作品名やジャンルなどの追加情報をいただければ、それに関連する情報をお伝えできるかもしれません。また一般的な映画の楽しみ方やおすすめジャンルについてのご質問にもお答えできます。\n",
      "\n",
      "--- MINIモード ---\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCancelledError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 60\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;66;03m# イベントループが既に実行中の場合\u001b[39;00m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m run_queries(rag, sample_queries)\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m:\n\u001b[32m     62\u001b[39m     \u001b[38;5;66;03m# 新しいループで実行\u001b[39;00m\n\u001b[32m     63\u001b[39m     asyncio.run(run_queries(rag, sample_queries))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 42\u001b[39m, in \u001b[36mrun_queries\u001b[39m\u001b[34m(rag_instance, queries)\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode.upper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mモード ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# 非同期でクエリを実行\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     answer = \u001b[38;5;28;01mawait\u001b[39;00m rag_instance.aquery(query, param=QueryParam(mode=mode))\n\u001b[32m     43\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m回答: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00manswer\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/app/minirag/minirag.py:549\u001b[39m, in \u001b[36mMiniRAG.aquery\u001b[39m\u001b[34m(self, query, param)\u001b[39m\n\u001b[32m    539\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m hybrid_query(\n\u001b[32m    540\u001b[39m         query,\n\u001b[32m    541\u001b[39m         \u001b[38;5;28mself\u001b[39m.chunk_entity_relation_graph,\n\u001b[32m   (...)\u001b[39m\u001b[32m    546\u001b[39m         asdict(\u001b[38;5;28mself\u001b[39m),\n\u001b[32m    547\u001b[39m     )\n\u001b[32m    548\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m param.mode == \u001b[33m\"\u001b[39m\u001b[33mmini\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m549\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m minirag_query(\n\u001b[32m    550\u001b[39m         query,\n\u001b[32m    551\u001b[39m         \u001b[38;5;28mself\u001b[39m.chunk_entity_relation_graph,\n\u001b[32m    552\u001b[39m         \u001b[38;5;28mself\u001b[39m.entities_vdb,\n\u001b[32m    553\u001b[39m         \u001b[38;5;28mself\u001b[39m.entity_name_vdb,\n\u001b[32m    554\u001b[39m         \u001b[38;5;28mself\u001b[39m.relationships_vdb,\n\u001b[32m    555\u001b[39m         \u001b[38;5;28mself\u001b[39m.chunks_vdb,\n\u001b[32m    556\u001b[39m         \u001b[38;5;28mself\u001b[39m.text_chunks,\n\u001b[32m    557\u001b[39m         \u001b[38;5;28mself\u001b[39m.embedding_func,\n\u001b[32m    558\u001b[39m         param,\n\u001b[32m    559\u001b[39m         asdict(\u001b[38;5;28mself\u001b[39m),\n\u001b[32m    560\u001b[39m     )\n\u001b[32m    561\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m param.mode == \u001b[33m\"\u001b[39m\u001b[33mnaive\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    562\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m naive_query(\n\u001b[32m    563\u001b[39m         query,\n\u001b[32m    564\u001b[39m         \u001b[38;5;28mself\u001b[39m.chunks_vdb,\n\u001b[32m   (...)\u001b[39m\u001b[32m    567\u001b[39m         asdict(\u001b[38;5;28mself\u001b[39m),\n\u001b[32m    568\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/app/minirag/operate.py:1425\u001b[39m, in \u001b[36mminirag_query\u001b[39m\u001b[34m(query, knowledge_graph_inst, entities_vdb, entity_name_vdb, relationships_vdb, chunks_vdb, text_chunks_db, embedder, query_param, global_config)\u001b[39m\n\u001b[32m   1423\u001b[39m TYPE_POOL, TYPE_POOL_w_CASE = \u001b[38;5;28;01mawait\u001b[39;00m knowledge_graph_inst.get_types()\n\u001b[32m   1424\u001b[39m kw_prompt = kw_prompt_temp.format(query=query, TYPE_POOL=TYPE_POOL)\n\u001b[32m-> \u001b[39m\u001b[32m1425\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m use_model_func(kw_prompt)\n\u001b[32m   1427\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1428\u001b[39m     keywords_data = json_repair.loads(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/app/minirag/utils.py:110\u001b[39m, in \u001b[36mlimit_async_func_call.<locals>.final_decro.<locals>.wait_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    108\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m asyncio.sleep(waitting_time)\n\u001b[32m    109\u001b[39m __current_size += \u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m func(*args, **kwargs)\n\u001b[32m    111\u001b[39m __current_size -= \u001b[32m1\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mopenrouter_openai_complete\u001b[39m\u001b[34m(prompt, system_prompt, history_messages, keyword_extraction, api_key, **kwargs)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mopenrouter_openai_complete\u001b[39m(\n\u001b[32m      2\u001b[39m     prompt,\n\u001b[32m      3\u001b[39m     system_prompt=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m      9\u001b[39m     \u001b[38;5;66;03m# if api_key:\u001b[39;00m\n\u001b[32m     10\u001b[39m     \u001b[38;5;66;03m#     os.environ[\"OPENROUTER_API_KEY\"] = api_key\u001b[39;00m\n\u001b[32m     12\u001b[39m     keyword_extraction = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mkeyword_extraction\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m openai_complete_if_cache(\n\u001b[32m     14\u001b[39m         LLM_MODEL,  \u001b[38;5;66;03m# change accordingly\u001b[39;00m\n\u001b[32m     15\u001b[39m         prompt,\n\u001b[32m     16\u001b[39m         system_prompt=system_prompt,\n\u001b[32m     17\u001b[39m         history_messages=history_messages,\n\u001b[32m     18\u001b[39m         base_url=\u001b[33m\"\u001b[39m\u001b[33mhttps://openrouter.ai/api/v1\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     19\u001b[39m         api_key=api_key,\n\u001b[32m     20\u001b[39m         **kwargs,\n\u001b[32m     21\u001b[39m     )\n\u001b[32m     22\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m keyword_extraction:  \u001b[38;5;66;03m# TODO: use JSON API\u001b[39;00m\n\u001b[32m     23\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m locate_json_string_body_from_string(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/app/minirag/llm/openai.py:127\u001b[39m, in \u001b[36mopenai_complete_if_cache\u001b[39m\u001b[34m(model, prompt, system_prompt, history_messages, base_url, api_key, **kwargs)\u001b[39m\n\u001b[32m    123\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m openai_async_client.beta.chat.completions.parse(\n\u001b[32m    124\u001b[39m         model=model, messages=messages, **kwargs\n\u001b[32m    125\u001b[39m     )\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m openai_async_client.chat.completions.create(\n\u001b[32m    128\u001b[39m         model=model, messages=messages, **kwargs\n\u001b[32m    129\u001b[39m     )\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(response, \u001b[33m\"\u001b[39m\u001b[33m__aiter__\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    133\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minner\u001b[39m():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py:2454\u001b[39m, in \u001b[36mAsyncCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   2411\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   2412\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   2413\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2451\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m   2452\u001b[39m ) -> ChatCompletion | AsyncStream[ChatCompletionChunk]:\n\u001b[32m   2453\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m2454\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m   2455\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m/chat/completions\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2456\u001b[39m         body=\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[32m   2457\u001b[39m             {\n\u001b[32m   2458\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: messages,\n\u001b[32m   2459\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m   2460\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33maudio\u001b[39m\u001b[33m\"\u001b[39m: audio,\n\u001b[32m   2461\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m\"\u001b[39m: frequency_penalty,\n\u001b[32m   2462\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfunction_call\u001b[39m\u001b[33m\"\u001b[39m: function_call,\n\u001b[32m   2463\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfunctions\u001b[39m\u001b[33m\"\u001b[39m: functions,\n\u001b[32m   2464\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogit_bias\u001b[39m\u001b[33m\"\u001b[39m: logit_bias,\n\u001b[32m   2465\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogprobs\u001b[39m\u001b[33m\"\u001b[39m: logprobs,\n\u001b[32m   2466\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_completion_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_completion_tokens,\n\u001b[32m   2467\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_tokens,\n\u001b[32m   2468\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m   2469\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodalities\u001b[39m\u001b[33m\"\u001b[39m: modalities,\n\u001b[32m   2470\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m: n,\n\u001b[32m   2471\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: parallel_tool_calls,\n\u001b[32m   2472\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprediction\u001b[39m\u001b[33m\"\u001b[39m: prediction,\n\u001b[32m   2473\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mpresence_penalty\u001b[39m\u001b[33m\"\u001b[39m: presence_penalty,\n\u001b[32m   2474\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mreasoning_effort\u001b[39m\u001b[33m\"\u001b[39m: reasoning_effort,\n\u001b[32m   2475\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mresponse_format\u001b[39m\u001b[33m\"\u001b[39m: response_format,\n\u001b[32m   2476\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mseed\u001b[39m\u001b[33m\"\u001b[39m: seed,\n\u001b[32m   2477\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mservice_tier\u001b[39m\u001b[33m\"\u001b[39m: service_tier,\n\u001b[32m   2478\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstop\u001b[39m\u001b[33m\"\u001b[39m: stop,\n\u001b[32m   2479\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstore\u001b[39m\u001b[33m\"\u001b[39m: store,\n\u001b[32m   2480\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: stream,\n\u001b[32m   2481\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream_options\u001b[39m\u001b[33m\"\u001b[39m: stream_options,\n\u001b[32m   2482\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: temperature,\n\u001b[32m   2483\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtool_choice\u001b[39m\u001b[33m\"\u001b[39m: tool_choice,\n\u001b[32m   2484\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m: tools,\n\u001b[32m   2485\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_logprobs\u001b[39m\u001b[33m\"\u001b[39m: top_logprobs,\n\u001b[32m   2486\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: top_p,\n\u001b[32m   2487\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m: user,\n\u001b[32m   2488\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mweb_search_options\u001b[39m\u001b[33m\"\u001b[39m: web_search_options,\n\u001b[32m   2489\u001b[39m             },\n\u001b[32m   2490\u001b[39m             completion_create_params.CompletionCreateParamsStreaming\n\u001b[32m   2491\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[32m   2492\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m completion_create_params.CompletionCreateParamsNonStreaming,\n\u001b[32m   2493\u001b[39m         ),\n\u001b[32m   2494\u001b[39m         options=make_request_options(\n\u001b[32m   2495\u001b[39m             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001b[32m   2496\u001b[39m         ),\n\u001b[32m   2497\u001b[39m         cast_to=ChatCompletion,\n\u001b[32m   2498\u001b[39m         stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   2499\u001b[39m         stream_cls=AsyncStream[ChatCompletionChunk],\n\u001b[32m   2500\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/openai/_base_client.py:1791\u001b[39m, in \u001b[36mAsyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1777\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1778\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1779\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1786\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1787\u001b[39m ) -> ResponseT | _AsyncStreamT:\n\u001b[32m   1788\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1789\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), **options\n\u001b[32m   1790\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1791\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/openai/_base_client.py:1526\u001b[39m, in \u001b[36mAsyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1524\u001b[39m response = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1525\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1526\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client.send(\n\u001b[32m   1527\u001b[39m         request,\n\u001b[32m   1528\u001b[39m         stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._should_stream_response_body(request=request),\n\u001b[32m   1529\u001b[39m         **kwargs,\n\u001b[32m   1530\u001b[39m     )\n\u001b[32m   1531\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m   1532\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mEncountered httpx.TimeoutException\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/httpx/_client.py:1629\u001b[39m, in \u001b[36mAsyncClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m   1625\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m   1627\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m-> \u001b[39m\u001b[32m1629\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_handling_auth(\n\u001b[32m   1630\u001b[39m     request,\n\u001b[32m   1631\u001b[39m     auth=auth,\n\u001b[32m   1632\u001b[39m     follow_redirects=follow_redirects,\n\u001b[32m   1633\u001b[39m     history=[],\n\u001b[32m   1634\u001b[39m )\n\u001b[32m   1635\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1636\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/httpx/_client.py:1657\u001b[39m, in \u001b[36mAsyncClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m   1654\u001b[39m request = \u001b[38;5;28;01mawait\u001b[39;00m auth_flow.\u001b[34m__anext__\u001b[39m()\n\u001b[32m   1656\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1657\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_handling_redirects(\n\u001b[32m   1658\u001b[39m         request,\n\u001b[32m   1659\u001b[39m         follow_redirects=follow_redirects,\n\u001b[32m   1660\u001b[39m         history=history,\n\u001b[32m   1661\u001b[39m     )\n\u001b[32m   1662\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1663\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/httpx/_client.py:1694\u001b[39m, in \u001b[36mAsyncClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m   1691\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m   1692\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m hook(request)\n\u001b[32m-> \u001b[39m\u001b[32m1694\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_single_request(request)\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1696\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/httpx/_client.py:1730\u001b[39m, in \u001b[36mAsyncClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1725\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1726\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an sync request with an AsyncClient instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1727\u001b[39m     )\n\u001b[32m   1729\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1730\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m transport.handle_async_request(request)\n\u001b[32m   1732\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, AsyncByteStream)\n\u001b[32m   1733\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/httpx/_transports/default.py:394\u001b[39m, in \u001b[36mAsyncHTTPTransport.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    381\u001b[39m req = httpcore.Request(\n\u001b[32m    382\u001b[39m     method=request.method,\n\u001b[32m    383\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    391\u001b[39m     extensions=request.extensions,\n\u001b[32m    392\u001b[39m )\n\u001b[32m    393\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m394\u001b[39m     resp = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pool.handle_async_request(req)\n\u001b[32m    396\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.AsyncIterable)\n\u001b[32m    398\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    399\u001b[39m     status_code=resp.status,\n\u001b[32m    400\u001b[39m     headers=resp.headers,\n\u001b[32m    401\u001b[39m     stream=AsyncResponseStream(resp.stream),\n\u001b[32m    402\u001b[39m     extensions=resp.extensions,\n\u001b[32m    403\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/httpcore/_async/connection_pool.py:256\u001b[39m, in \u001b[36mAsyncConnectionPool.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.AsyncIterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/httpcore/_async/connection_pool.py:236\u001b[39m, in \u001b[36mAsyncConnectionPool.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = \u001b[38;5;28;01mawait\u001b[39;00m pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m connection.handle_async_request(\n\u001b[32m    237\u001b[39m         pool_request.request\n\u001b[32m    238\u001b[39m     )\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/httpcore/_async/connection.py:103\u001b[39m, in \u001b[36mAsyncHTTPConnection.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection.handle_async_request(request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/httpcore/_async/http11.py:136\u001b[39m, in \u001b[36mAsyncHTTP11Connection.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/httpcore/_async/http11.py:106\u001b[39m, in \u001b[36mAsyncHTTP11Connection.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._receive_response_headers(**kwargs)\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/httpcore/_async/http11.py:177\u001b[39m, in \u001b[36mAsyncHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._receive_event(timeout=timeout)\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/httpcore/_async/http11.py:217\u001b[39m, in \u001b[36mAsyncHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._network_stream.read(\n\u001b[32m    218\u001b[39m         \u001b[38;5;28mself\u001b[39m.READ_NUM_BYTES, timeout=timeout\n\u001b[32m    219\u001b[39m     )\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/httpcore/_backends/anyio.py:35\u001b[39m, in \u001b[36mAnyIOStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m anyio.fail_after(timeout):\n\u001b[32m     34\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stream.receive(max_bytes=max_bytes)\n\u001b[32m     36\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m anyio.EndOfStream:  \u001b[38;5;66;03m# pragma: nocover\u001b[39;00m\n\u001b[32m     37\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/anyio/streams/tls.py:219\u001b[39m, in \u001b[36mTLSStream.receive\u001b[39m\u001b[34m(self, max_bytes)\u001b[39m\n\u001b[32m    218\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreceive\u001b[39m(\u001b[38;5;28mself\u001b[39m, max_bytes: \u001b[38;5;28mint\u001b[39m = \u001b[32m65536\u001b[39m) -> \u001b[38;5;28mbytes\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m     data = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_sslobject_method(\u001b[38;5;28mself\u001b[39m._ssl_object.read, max_bytes)\n\u001b[32m    220\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[32m    221\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m EndOfStream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/anyio/streams/tls.py:162\u001b[39m, in \u001b[36mTLSStream._call_sslobject_method\u001b[39m\u001b[34m(self, func, *args)\u001b[39m\n\u001b[32m    159\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._write_bio.pending:\n\u001b[32m    160\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transport_stream.send(\u001b[38;5;28mself\u001b[39m._write_bio.read())\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m     data = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transport_stream.receive()\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m EndOfStream:\n\u001b[32m    164\u001b[39m     \u001b[38;5;28mself\u001b[39m._read_bio.write_eof()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/anyio/_backends/_asyncio.py:1254\u001b[39m, in \u001b[36mSocketStream.receive\u001b[39m\u001b[34m(self, max_bytes)\u001b[39m\n\u001b[32m   1248\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   1249\u001b[39m     \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._protocol.read_event.is_set()\n\u001b[32m   1250\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._transport.is_closing()\n\u001b[32m   1251\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._protocol.is_at_eof\n\u001b[32m   1252\u001b[39m ):\n\u001b[32m   1253\u001b[39m     \u001b[38;5;28mself\u001b[39m._transport.resume_reading()\n\u001b[32m-> \u001b[39m\u001b[32m1254\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._protocol.read_event.wait()\n\u001b[32m   1255\u001b[39m     \u001b[38;5;28mself\u001b[39m._transport.pause_reading()\n\u001b[32m   1256\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/asyncio/locks.py:213\u001b[39m, in \u001b[36mEvent.wait\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    211\u001b[39m \u001b[38;5;28mself\u001b[39m._waiters.append(fut)\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m fut\n\u001b[32m    214\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[31mCancelledError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# 約3分かかった\n",
    "\n",
    "# # サンプルクエリ\n",
    "# queries = [\n",
    "#     \"映画について教えて\",\n",
    "#     \"仕事のプロジェクトはどうでしたか？\",\n",
    "#     \"料理で何を作りましたか？\",\n",
    "#     \"読んでいる本について教えて\",\n",
    "#     \"散歩について詳しく教えて\"\n",
    "# ]\n",
    "\n",
    "# # 各モードでクエリを実行。この3つがある\n",
    "# modes = [\"naive\", \"mini\", \"light\"]\n",
    "\n",
    "# for query in queries:\n",
    "#     print(f\"\\n{'='*50}\")\n",
    "#     print(f\"クエリ: {query}\")\n",
    "#     print(f\"{'='*50}\")\n",
    "\n",
    "#     for mode in modes:\n",
    "#         print(f\"\\n--- {mode.upper()}モード ---\")\n",
    "#         try:\n",
    "#             answer = rag.query(query, param=QueryParam(mode=mode))     # .replace(\"\\n\", \"\").replace(\"\\r\", \"\")\n",
    "#             print(f\"回答: {answer}\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"エラー: {e}\")\n",
    "\n",
    "\n",
    "async def run_queries(rag_instance, queries):\n",
    "    # 各モードでクエリを実行。この3つがある\n",
    "    modes = [\"naive\", \"mini\", \"light\"]\n",
    "\n",
    "    for query in queries:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"クエリ: {query}\")\n",
    "        print(f\"{'='*50}\")\n",
    "\n",
    "        for mode in modes:\n",
    "            print(f\"\\n--- {mode.upper()}モード ---\")\n",
    "            try:\n",
    "                # 非同期でクエリを実行\n",
    "                answer = await rag_instance.aquery(query, param=QueryParam(mode=mode))\n",
    "                print(f\"回答: {answer}\")\n",
    "            except Exception as e:\n",
    "                print(f\"エラー: {e}\")\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "sample_queries = [\n",
    "    \"映画について教えて\",\n",
    "    \"仕事のプロジェクトはどうでしたか？\",\n",
    "    \"料理で何を作りましたか？\",\n",
    "    \"読んでいる本について教えて\",\n",
    "    \"散歩について詳しく教えて\"\n",
    "]\n",
    "\n",
    "# イベントループが既に実行中の場合\n",
    "try:\n",
    "    await run_queries(rag, sample_queries)\n",
    "except RuntimeError:\n",
    "    # 新しいループで実行\n",
    "    asyncio.run(run_queries(rag, sample_queries))\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"処理時間: {elapsed_time:.4f}秒\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5621b47-01eb-468b-9e18-6d273ab147f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "1/0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8091840-cd5f-4ce8-a6e8-d770b4e47e6e",
   "metadata": {},
   "source": [
    "### コード上にバグ「status=$1」があるのでDockerファイル上で修正した\n",
    "- sed -i 's/where workspace=$1 and status=$1/where workspace=$1 and status=$2/' minirag/kg/postgres_impl.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35f6d297-3666-4efa-9232-70b2328abfa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------- モデルとトークナイザーを読み込みます -------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at sentence-transformers/all-MiniLM-L6-v2 and are newly initialized: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PostgreSQL database error: relation \"lightrag_doc_full\" does not exist\n",
      "Failed to check table LIGHTRAG_DOC_FULL in PostgreSQL database\n",
      "PostgreSQL database error: relation \"lightrag_doc_full\" does not exist\n",
      "PostgreSQL database error: relation \"lightrag_doc_chunks\" does not exist\n",
      "Failed to check table LIGHTRAG_DOC_CHUNKS in PostgreSQL database\n",
      "PostgreSQL database error: relation \"lightrag_doc_chunks\" does not exist\n",
      "PostgreSQL database error: relation \"lightrag_vdb_entity\" does not exist\n",
      "Failed to check table LIGHTRAG_VDB_ENTITY in PostgreSQL database\n",
      "PostgreSQL database error: relation \"lightrag_vdb_entity\" does not exist\n",
      "PostgreSQL database error: relation \"lightrag_vdb_relation\" does not exist\n",
      "Failed to check table LIGHTRAG_VDB_RELATION in PostgreSQL database\n",
      "PostgreSQL database error: relation \"lightrag_vdb_relation\" does not exist\n",
      "PostgreSQL database error: relation \"lightrag_llm_cache\" does not exist\n",
      "Failed to check table LIGHTRAG_LLM_CACHE in PostgreSQL database\n",
      "PostgreSQL database error: relation \"lightrag_llm_cache\" does not exist\n",
      "PostgreSQL database error: relation \"lightrag_doc_status\" does not exist\n",
      "Failed to check table LIGHTRAG_DOC_STATUS in PostgreSQL database\n",
      "PostgreSQL database error: relation \"lightrag_doc_status\" does not exist\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------- モデルとトークナイザーが解放されました！ -------------------------\n",
      "------------------------- ポスグレに接続しています -------------------------\n",
      "------------------------- ポスグレに接続しました！ -------------------------\n",
      "------------------------- テーブルの存在を確認・作成しています -------------------------\n",
      "SELECT 1 FROM LIGHTRAG_DOC_FULL LIMIT 1\n",
      "None\n",
      "SELECT 1 FROM LIGHTRAG_DOC_CHUNKS LIMIT 1\n",
      "None\n",
      "SELECT 1 FROM LIGHTRAG_VDB_ENTITY LIMIT 1\n",
      "None\n",
      "SELECT 1 FROM LIGHTRAG_VDB_RELATION LIMIT 1\n",
      "None\n",
      "SELECT 1 FROM LIGHTRAG_LLM_CACHE LIMIT 1\n",
      "None\n",
      "SELECT 1 FROM LIGHTRAG_DOC_STATUS LIMIT 1\n",
      "None\n",
      "------------------------- テーブルの準備が完了しました！ -------------------------\n",
      "------------------------- MiniRAGが初期化されました！ -------------------------\n",
      "データを挿入中...\n",
      "テキスト 1/4 を挿入中...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 1/1 [00:00<00:00,  1.56batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠙ Processed 1 chunks, 11 entities(duplicated), 6 relations(duplicated)\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "Key value duplicate, but upsert succeeded.\n",
      "Key value duplicate, but upsert succeeded.\n",
      "Key value duplicate, but upsert succeeded.\n",
      "Key value duplicate, but upsert succeeded.\n",
      "Key value duplicate, but upsert succeeded.\n",
      "Key value duplicate, but upsert succeeded.\n",
      "Key value duplicate, but upsert succeeded.\n",
      "Key value duplicate, but upsert succeeded.\n",
      "Key value duplicate, but upsert succeeded.\n",
      "Key value duplicate, but upsert succeeded.\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "Key value duplicate, but upsert succeeded.\n",
      "Key value duplicate, but upsert succeeded.\n",
      "Key value duplicate, but upsert succeeded.\n",
      "Key value duplicate, but upsert succeeded.\n",
      "Key value duplicate, but upsert succeeded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings:   0%|          | 0/1 [00:00<?, ?batch/s]We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:01<00:00,  1.37s/batch]\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:00<00:00,  1.48batch/s]\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:01<00:00,  1.13s/batch]\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:01<00:00,  1.13s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KG successfully indexed.\n",
      "テキスト 2/4 を挿入中...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 1/1 [00:01<00:00,  1.37s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠹ Processed 2 chunks, 16 entities(duplicated), 6 relations(duplicated)\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 1/1 [00:00<00:00,  1.43batch/s]\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:00<00:00,  1.40batch/s]\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:01<00:00,  1.14s/batch]\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:01<00:00,  1.11s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KG successfully indexed.\n",
      "テキスト 3/4 を挿入中...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 1/1 [00:01<00:00,  1.10s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠸ Processed 3 chunks, 23 entities(duplicated), 15 relations(duplicated)\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 1/1 [00:00<00:00,  1.27batch/s]\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:01<00:00,  1.64s/batch]\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:01<00:00,  1.21s/batch]\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:00<00:00,  1.50batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KG successfully indexed.\n",
      "テキスト 4/4 を挿入中...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 1/1 [00:00<00:00,  1.45batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠼ Processed 4 chunks, 20 entities(duplicated), 14 relations(duplicated)\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 1/1 [00:01<00:00,  1.13s/batch]\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:00<00:00,  1.28batch/s]\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:01<00:00,  1.14s/batch]\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:01<00:00,  1.63s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KG successfully indexed.\n",
      "\n",
      "すべてのデータが挿入されました！\n",
      "処理時間: 190.0882秒\n",
      "\n",
      "==================================================\n",
      "クエリ: 映画について教えて\n",
      "==================================================\n",
      "\n",
      "--- NAIVEモード ---\n",
      "回答: ### 映画に関する情報\n",
      "\n",
      "提供された文書から、以下の映画に関する情報がわかります：\n",
      "\n",
      "**「君の名は。」**  \n",
      "- 友人と一緒に映画館で鑑賞しました  \n",
      "- 非常に感動的な作品だったという印象を持たれたようです  \n",
      "- 観覧した日の全体的な流れの中では「素晴らしい一日」の一部として位置付けられており、その日の良い思い出の一部となっていることが伺えます\n",
      "\n",
      "### その他の情報\n",
      "\n",
      "この映画についての詳細な感想（ストーリーの展開やキャラクターについてなど）や技術的な側面（映像美や音楽など）に関する具体的な言及は文書に含まれていません。また、他の映画についての言及も現在の文書には見当たりません。\n",
      "\n",
      "この作品は新海誠監督によるアニメーション映画で、一般的に評価の高い作品として知られていますが、文書内ではそうした背景情報への言及は特にありません。\n",
      "\n",
      "--- MINIモード ---\n",
      "エラー: \n",
      "\n",
      "--- LIGHTモード ---\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "回答: # 映画「君の名は。」についての記録\n",
      "\n",
      "## 映画鑑賞体験\n",
      "\n",
      "記録によると、作者は友人と一緒に映画「君の名は。」を鑑賞しました。この体験は「とても感動的でした」と表現されており、特に印象深い出来事として記録されています。映画鑑賞は「一日の素晴らしい出来事」の一部として回想されており、午後の時間を使って行われたことが分かります。\n",
      "\n",
      "## 作品の特性\n",
      "\n",
      "「君の名は。」は観客に強い感情的な影響を与える作品として描写されています。作者が「感動的」と繰り返し述べていることから、この映画は感情に訴えかける力が強い作品であることが窺えます。友人との共有体験としての側面も強調されており、社会的な交流を伴う娯楽体験としての価値があったようです。\n",
      "\n",
      "## 関連情報\n",
      "\n",
      "この映画鑑賞は、朝に公園で桜の満開を楽しんだ散歩と並んで、その日の主な出来事として記憶されています。公園の美しい風景と映画の感動的なストーリーが、一日の特別な体験を作り出したことが示唆されています。\n",
      "\n",
      "以上がデータから得られる「君の名は。」に関する情報の全てです。これ以上の詳細な情報は提供されたデータには含まれていません。\n",
      "\n",
      "==================================================\n",
      "クエリ: 仕事のプロジェクトはどうでしたか？\n",
      "==================================================\n",
      "\n",
      "--- NAIVEモード ---\n",
      "回答: ### 仕事のプロジェクトの成果  \n",
      "\n",
      "あなたが取り組まれていたAIシステムの開発プロジェクトは、**無事に完了**しました。このプロジェクトには**チーム全員で3ヶ月間**取り組み、特に注目すべき点として、**機械学習モデルの精度が95%を超えた**ことが挙げられます。この成果は非常に高く評価され、**クライアントからも高い評価をいただいた**とのことです。  \n",
      "\n",
      "このような成功は、チームの結束力と努力の結果と言えるでしょう。AIシステムの開発は複雑で時間がかかる作業ですが、目標を達成できたことは大きな喜びです。特に機械学習の精度が95%を超えたことは、技術的な裏付けと実用性の両面で優れた成果であったと考えられます。  \n",
      "\n",
      "今後のプロジェクトにも、この経験を活かしていけると良いですね。\n",
      "\n",
      "--- MINIモード ---\n",
      "エラー: \n",
      "\n",
      "--- LIGHTモード ---\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "回答: ## 3ヶ月間のAIシステム開発プロジェクトについて\n",
      "\n",
      "情報を整理しますと、チーム全員で3ヶ月間にわたって取り組んだAIシステムの開発プロジェクトが無事完了しました。主要な成果として、以下の点が報告されています：\n",
      "\n",
      "- **高い技術的成果**: AIシステムの核心となる機械学習モデルが95%以上の精度を達成しました。この高い精度がプロジェクト全体の品質を大きく向上させた要因となりました。\n",
      "\n",
      "- **クライアント評価**: 完成したAIシステムはクライアントから非常に高い評価を受けました。精度95%という数値がその評価の主な根拠となったようです。\n",
      "\n",
      "- **チームワーク**: このプロジェクトの成功は、3ヶ月間献身的に取り組んだチーム全体の努力によるものでした。複数のデータソースでもチームへの言及があり、チームワークが重視されていたことがわかります。\n",
      "\n",
      "## 技術的詳細\n",
      "\n",
      "特に注目すべきは機械学習モデルの性能で、これはAIシステム全体の中心的な技術コンポーネントとして開発されました。「95%以上の精度」という具体的な指標が達成されたことで、プロジェクトの技術的な成功が明白になっています。\n",
      "\n",
      "この開発プロジェクトは、技術実装だけでなくクライアント評価というビジネス面でも成功を収めたと言えるでしょう。\n",
      "\n",
      "==================================================\n",
      "クエリ: 料理で何を作りましたか？\n",
      "==================================================\n",
      "\n",
      "--- NAIVEモード ---\n",
      "回答: ### 料理の挑戦について\n",
      "\n",
      "週末に**初めてパスタを一から作る**ことに挑戦しました。特に、**小麦粉から麺を作る工程**にチャレンジしたようで、思っていたよりも難しかったとのことです。しかし、努力の甲斐あって、最終的に**美味しいカルボナーラ**が完成したとの報告がありました。  \n",
      "\n",
      "### 今後の料理計画  \n",
      "\n",
      "次回は**リゾット**を作る予定のようです。初めてのパスタ作りを成功させた経験から、今後の料理への意欲も感じられます。新しいレシピに挑戦する姿勢は素晴らしいですね！  \n",
      "\n",
      "パスタ作りの難しさと達成感、そして次への意欲が伝わる内容でした。\n",
      "\n",
      "--- MINIモード ---\n",
      "エラー: \n",
      "\n",
      "--- LIGHTモード ---\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "回答: 週末に、料理に挑戦したことが記録されています。具体的には、**パスタを一から作る**ことに取り組んだようです。小麦粉から麺を作る工程に挑戦しましたが、思っていたよりも難しかったと感想が述べられています。最終的には、**美味しいカルボナーラ**が完成したとのことです。また、次回は**リゾット**を作ってみたいという意欲も記されています。  \n",
      "\n",
      "この記述から、料理のスキル向上への関心や、新しいレシピへの挑戦意欲が感じられます。特に、食材から手作りするこだわりや、完成した料理への満足感が伝わってきます。\n",
      "\n",
      "==================================================\n",
      "クエリ: 読んでいる本について教えて\n",
      "==================================================\n",
      "\n",
      "--- NAIVEモード ---\n",
      "回答: ### 読書に関する情報\n",
      "\n",
      "現在、2冊の本を読まれているようです。\n",
      "\n",
      "1. **「ノルウェイの森」村上春樹**  \n",
      "   この小説については、特に主人公の心情描写の繊細さに引き込まれているとのことです。村上春樹の作風は内面の描写に優れており、読者の共感を誘う作品として知られています。\n",
      "\n",
      "2. **技術書「深層学習」**  \n",
      "   具体的な書名は不明ですが、深層学習に関する技術書を読んでいるとのことです。理論と実践のバランスが取れた内容に触れられていますが、詳細な内容までは記載されていません。深層学習はAIの主要な分野であり、機械学習モデルの開発（95%の精度を達成したとの前文から推測）にも関連するテーマといえます。\n",
      "\n",
      "### その他の趣味活動\n",
      "\n",
      "- **料理**：最近はパスタの自家製麺に挑戦し、カルボナーラを作ったそうです。次はリゾットに挑戦する予定とのこと。  \n",
      "- **映画鑑賞**：友人と「君の名は。」を観て感動したとの記載があります。\n",
      "\n",
      "文学と技術書という一見対照的な読書傾向が、創造性と論理性のバランスを表しているように思われます。AI開発に携わる一方で、小説を通じて情感にも触れる時間を大切にしているのでしょう。\n",
      "\n",
      "--- MINIモード ---\n",
      "エラー: \n",
      "\n",
      "--- LIGHTモード ---\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "回答: 提供されたデータテーブルには、書籍やその内容に関する具体的な情報が含まれていません。データテーブルには「Entities」「Relationships」「Sources」という3つのセクションがありますが、それぞれの構造を示すヘッダー情報しか記載されておらず、実際の書籍データや関連情報は確認できません。\n",
      "\n",
      "現時点でわかる範囲では：\n",
      "- **Entitiesテーブル**：このテーブルでは「id」「entity」「type」「description」「rank」という列が存在しますが、具体的な書籍や著者などのエンティティ（実体）データは含まれていません。\n",
      "- **Relationshipsテーブル**：このテーブルでは「id」「source」「target」「description」「keywords」「weight」「rank」という列がありますが、書籍間の関連性や主題のつながりを示すデータは提供されていません。\n",
      "- **Sourcesテーブル**：最も簡素な構造で「id」と「content」のみが定義されていますが、実際の参考文献や出典内容は記載されていません。\n",
      "\n",
      "**ご質問への回答として**  \n",
      "現状のデータのみでは、どのような本について話されているのか、またはその内容を分析することが不可能です。もし特定の書籍やトピックについての情報が必要でしたら、より詳細なデータ（例えば書籍のタイトル、著者、あらすじなどが記載された具体的なテーブル内容）を提供していただければ、さらに詳しい分析や解説が可能です。  \n",
      "\n",
      "データが不足しているため、一般的な書籍情報の探し方として、図書館のオンラインカタログや書評サイト（例えばGoodreads）を活用することを提案します。\n",
      "\n",
      "==================================================\n",
      "クエリ: 散歩について詳しく教えて\n",
      "==================================================\n",
      "\n",
      "--- NAIVEモード ---\n",
      "回答: ### 公園の散歩について  \n",
      "\n",
      "昨日の朝、作者は近所の公園を散歩したようです。その時の情景として、**桜の花が満開で、とても美しかった**と描写されています。春の季節を感じさせる穏やかな光景が思い浮かびます。  \n",
      "\n",
      "### 散歩の時間帯とその後の予定  \n",
      "散歩は**朝早く**に行われたようです。午後には友人と映画「君の名は。」を観に行き、夜は自宅で過ごしたとのことです。朝の散歩が一日の良いスタートとなり、充実した一日を過ごせた様子が伺えます。  \n",
      "\n",
      "残念ながら、公園の具体的な名前や散歩コースの詳細、どのくらいの時間歩いたかといった情報は提供されていません。もしそれらの情報があれば、さらに詳しく描写できたかもしれません。  \n",
      "\n",
      "全体的に、作者は自然を楽しむことを大切にしているようで、その穏やかで感動的な瞬間を大切にしていることが伝わってきます。\n",
      "\n",
      "--- MINIモード ---\n",
      "エラー: \n",
      "\n",
      "--- LIGHTモード ---\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "★デバッグ: create_graph already exists\n",
      "回答: ### 散歩についての詳細\n",
      "\n",
      "散歩は、朝早く近所の公園で行われた活動として記述されています。公園は桜の花が満開で美しい景観が広がっており、自然と触れ合う時間として楽しんだようです。この散歩は朝のルーティンの一部として行われ、一日の始まりを清々しい気分でスタートさせる役割を果たしました。\n",
      "\n",
      "### 散歩の関連情報\n",
      "\n",
      "- **場所**: 近所の公園。桜の花が満開で、美しい風景が広がっていたことが強調されています。\n",
      "- **時間帯**: 朝早く。早起きして散歩を楽しんだことがわかります。\n",
      "- **関連する出来事**: 同じ日に午後には友人と映画「君の名は。」を鑑賞しており、一日を通してリラックスした時間を過ごしたようです。\n",
      "\n",
      "散歩は、自然との触れ合いや健康のために行われるだけでなく、一日の他の楽しい活動と組み合わさって、充実した日々を構成する要素の一つとして描かれています。\n",
      "処理時間: 207.5090秒\n"
     ]
    }
   ],
   "source": [
    "# どのエラーでリトライするかを定義\n",
    "# 429 (RateLimitError) や サーバー側のエラー(5xx) でリトライするのが一般的\n",
    "def should_retry(e: Exception) -> bool:\n",
    "    if isinstance(e, RateLimitError):\n",
    "        print(f\"RateLimitError発生。リトライします...: {e}\")\n",
    "        return True\n",
    "    # 5xx系のサーバーエラーでもリトライすることが多い\n",
    "    if isinstance(e, APIStatusError) and e.status_code >= 500:\n",
    "        print(f\"サーバーエラー( {e.status_code} )発生。リトライします...: {e}\")\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "@retry(\n",
    "    wait=wait_random_exponential(multiplier=1, max=70), # 1, 2, 4, 8秒...とランダムな時間を加えて待つ（最大70秒）\n",
    "    stop=stop_after_attempt(5), # 最大5回リトライする\n",
    "    retry=retry_if_exception(should_retry) # 上で定義した条件の例外が発生した場合にリトライ\n",
    ")\n",
    "async def openrouter_openai_complete(\n",
    "    prompt,\n",
    "    system_prompt=None,\n",
    "    history_messages=[],\n",
    "    keyword_extraction=False,\n",
    "    api_key: str = None,\n",
    "    **kwargs,\n",
    ") -> str:\n",
    "    # if api_key:\n",
    "    #     os.environ[\"OPENROUTER_API_KEY\"] = api_key\n",
    "\n",
    "    keyword_extraction = kwargs.pop(\"keyword_extraction\", None)\n",
    "    result = await openai_complete_if_cache(\n",
    "        LLM_MODEL,  # change accordingly\n",
    "        prompt,\n",
    "        system_prompt=system_prompt,\n",
    "        history_messages=history_messages,\n",
    "        base_url=\"https://openrouter.ai/api/v1\",\n",
    "        api_key=api_key,\n",
    "        **kwargs,\n",
    "    )\n",
    "    if keyword_extraction:  # TODO: use JSON API\n",
    "        return locate_json_string_body_from_string(result)\n",
    "    return result\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "print(\"------------------------- モデルとトークナイザーを読み込みます -------------------------\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL, token=HF_TOKEN)\n",
    "model = AutoModelForMaskedLM.from_pretrained(EMBEDDING_MODEL, token=HF_TOKEN)\n",
    "\n",
    "\n",
    "# メモリ解放のためにモデルを削除\n",
    "del model\n",
    "del tokenizer\n",
    "\n",
    "# PyTorchのキャッシュをクリア (GPUを使用している場合)\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Pythonのガベージコレクションを実行\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "print(\"------------------------- モデルとトークナイザーが解放されました！ -------------------------\")\n",
    "\n",
    "\n",
    "import pipmaster as pm\n",
    "if not pm.is_installed(\"psycopg-pool\"):\n",
    "    pm.install(\"psycopg-pool\")\n",
    "    pm.install(\"psycopg[binary,pool]\")\n",
    "if not pm.is_installed(\"asyncpg\"):\n",
    "    pm.install(\"asyncpg\")\n",
    "\n",
    "import asyncpg\n",
    "import psycopg\n",
    "from psycopg_pool import AsyncConnectionPool\n",
    "from minirag.kg.postgres_impl import PostgreSQLDB\n",
    "from minirag.kg.postgres_impl import PGKVStorage\n",
    "from minirag.kg.postgres_impl import PGVectorStorage\n",
    "from minirag.kg.postgres_impl import PGGraphStorage\n",
    "from minirag.kg.postgres_impl import PGDocStatusStorage\n",
    "\n",
    "async def setup_rag_system():\n",
    "    \"\"\"RAGシステムを初期化し、準備ができたインスタンスを返す\"\"\"\n",
    "\n",
    "    # 1. データベース設定\n",
    "    db_config={\n",
    "        \"host\": \"postgres\",    # これは Docker のサービス名「postgres」で指定\n",
    "        \"port\": 5432,\n",
    "        \"user\": os.getenv(\"POSTGRES_USER\"),\n",
    "        \"password\": os.getenv(\"POSTGRES_PASSWORD\"),\n",
    "        \"database\": os.getenv(\"POSTGRES_DB\"),\n",
    "    }\n",
    "\n",
    "    # 2. PostgreSQLDBインスタンスを作成\n",
    "    db_postgre = PostgreSQLDB(config=db_config)\n",
    "\n",
    "    # 3. データベース接続を初期化\n",
    "    print(\"------------------------- ポスグレに接続しています -------------------------\")\n",
    "    await db_postgre.initdb()\n",
    "    print(\"------------------------- ポスグレに接続しました！ -------------------------\")\n",
    "\n",
    "    # 必要なテーブルが存在するかチェックし、なければ作成する\n",
    "    print(\"------------------------- テーブルの存在を確認・作成しています -------------------------\")\n",
    "    await db_postgre.check_tables()\n",
    "    print(\"------------------------- テーブルの準備が完了しました！ -------------------------\")\n",
    "\n",
    "    # 5. MiniRAGインスタンスを作成\n",
    "    os.environ[\"AGE_GRAPH_NAME\"] = \"my_minirag_graph\" # または設定から取得します\n",
    "    rag = MiniRAG(\n",
    "        working_dir=WORKING_DIR,\n",
    "    \n",
    "        # ポスグレ\n",
    "        # kv_storage=\"PGKVStorage\",\n",
    "        # vector_storage=\"PGVectorStorage\",\n",
    "        # graph_storage=\"PGGraphStorage\",\n",
    "    \n",
    "        # llm_model_func=hf_model_complete,\n",
    "        # llm_model_func=gemini_2_5_flash_complete,\n",
    "        llm_model_func=openrouter_openai_complete,\n",
    "    \n",
    "        llm_model_max_token_size=200,\n",
    "        llm_model_name=LLM_MODEL,\n",
    "        embedding_func=EmbeddingFunc(\n",
    "            embedding_dim=EMBEDDING_DIM,\n",
    "            max_token_size=1000,\n",
    "            func=lambda texts: hf_embed(\n",
    "                texts,\n",
    "                tokenizer=AutoTokenizer.from_pretrained(EMBEDDING_MODEL),\n",
    "                embed_model=AutoModel.from_pretrained(EMBEDDING_MODEL),\n",
    "            ),\n",
    "        ),\n",
    "        kv_storage=\"PGKVStorage\",\n",
    "        vector_storage=\"PGVectorStorage\",\n",
    "        graph_storage=\"PGGraphStorage\",\n",
    "        doc_status_storage=\"PGDocStatusStorage\",\n",
    "        vector_db_storage_cls_kwargs={\n",
    "            \"cosine_better_than_threshold\": float(os.getenv(\"COSINE_THRESHOLD\"))\n",
    "        }\n",
    "    )\n",
    "    # データベースの情報を渡す\n",
    "    rag.set_storage_client(db_postgre)    \n",
    "    return rag\n",
    "\n",
    "# RAGシステムをセットアップ\n",
    "try:\n",
    "    rag = await setup_rag_system()\n",
    "    print(\"------------------------- MiniRAGが初期化されました！ -------------------------\")\n",
    "except Exception as e:\n",
    "    print(f\"RAGシステムのセットアップに失敗しました: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 約12分かかった\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# サンプルテキストデータ\n",
    "sample_texts = [\n",
    "    \"\"\"\n",
    "今日は素晴らしい一日でした。朝早く起きて、近所の公園を散歩しました。\n",
    "桜の花が満開で、とても美しかったです。午後は友人と映画を見に行きました。\n",
    "「君の名は。」という映画で、とても感動的でした。\n",
    "夜は家族と一緒に夕食を取り、楽しい時間を過ごしました。\n",
    "\"\"\",\n",
    "    \"\"\"\n",
    "昨日は仕事で大きなプロジェクトが完了しました。\n",
    "チーム全員で3ヶ月間取り組んできたAIシステムの開発が終わりました。\n",
    "機械学習モデルの精度が95%を超え、クライアントからも高い評価をいただきました。\n",
    "今夜はチームメンバーと祝賀会を開く予定です。\n",
    "\"\"\",\n",
    "    \"\"\"\n",
    "週末は料理に挑戦しました。初めてパスタを一から作ってみました。\n",
    "小麦粉から麺を作るのは思っていたより難しかったですが、\n",
    "最終的にはとても美味しいカルボナーラができました。\n",
    "次回はリゾットに挑戦してみたいと思います。\n",
    "\"\"\",\n",
    "    \"\"\"\n",
    "読書が趣味で、最近は村上春樹の「ノルウェイの森」を読んでいます。\n",
    "主人公の心情描写がとても繊細で、引き込まれます。\n",
    "また、技術書も読んでおり、「深層学習」について学んでいます。\n",
    "理論と実践のバランスが取れた良い本だと思います。\n",
    "\"\"\"\n",
    "]\n",
    "\n",
    "# データの挿入\n",
    "print(\"データを挿入中...\")\n",
    "\n",
    "async def insert_texts(rag_instance, texts):\n",
    "    for i, text in enumerate(texts):\n",
    "        print(f\"テキスト {i+1}/{len(texts)} を挿入中...\")\n",
    "        await rag_instance.ainsert(text.strip())\n",
    "\n",
    "    print(\"\\nすべてのデータが挿入されました！\")\n",
    "\n",
    "\n",
    "# イベントループが既に実行中の場合\n",
    "try:\n",
    "    await insert_texts(rag, sample_texts)\n",
    "except RuntimeError:\n",
    "    # 新しいループで実行\n",
    "    asyncio.run(insert_texts(rag, sample_texts))\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"処理時間: {elapsed_time:.4f}秒\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "async def run_queries(rag_instance, queries):\n",
    "    # 各モードでクエリを実行。この3つがある\n",
    "    modes = [\"naive\", \"mini\", \"light\"]\n",
    "\n",
    "    for query in queries:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"クエリ: {query}\")\n",
    "        print(f\"{'='*50}\")\n",
    "\n",
    "        for mode in modes:\n",
    "            print(f\"\\n--- {mode.upper()}モード ---\")\n",
    "            try:\n",
    "                # 非同期でクエリを実行\n",
    "                answer = await rag_instance.aquery(query, param=QueryParam(mode=mode))\n",
    "                print(f\"回答: {answer}\")\n",
    "            except Exception as e:\n",
    "                print(f\"エラー: {e}\")\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "sample_queries = [\n",
    "    \"映画について教えて\",\n",
    "    \"仕事のプロジェクトはどうでしたか？\",\n",
    "    \"料理で何を作りましたか？\",\n",
    "    \"読んでいる本について教えて\",\n",
    "    \"散歩について詳しく教えて\"\n",
    "]\n",
    "\n",
    "# イベントループが既に実行中の場合\n",
    "try:\n",
    "    await run_queries(rag, sample_queries)\n",
    "except RuntimeError:\n",
    "    # 新しいループで実行\n",
    "    asyncio.run(run_queries(rag, sample_queries))\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"処理時間: {elapsed_time:.4f}秒\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cb1326-c4af-4dd5-90b6-23c5d7308051",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303a40d0-373b-48d7-b587-d2fd80e5e063",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514a9e0c-57d3-42eb-88bf-4f2645bd0da5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6cf0e2-f120-4317-b6d2-e961686bbe8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc05b4c4-ff34-4eb6-b31a-8fdb36b8e246",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514bc9a6-471c-4ebc-a3ae-9aa375f3c35c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d39171f-f00b-4e3a-8819-df013f50008e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PGVectorStorage(namespace='entities', global_config={'working_dir': '/tmp/minirag_demo', 'kv_storage': 'PGKVStorage', 'vector_storage': 'PGVectorStorage', 'graph_storage': 'PGGraphStorage', 'log_level': 0, 'chunk_token_size': 1200, 'chunk_overlap_token_size': 100, 'tiktoken_model_name': 'gpt-4o-mini', 'entity_extract_max_gleaning': 1, 'entity_summary_to_max_tokens': 500, 'node_embedding_algorithm': 'node2vec', 'node2vec_params': {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3}, 'embedding_func': <function limit_async_func_call.<locals>.final_decro.<locals>.wait_func at 0x79ef73377b00>, 'embedding_batch_num': 32, 'embedding_func_max_async': 16, 'llm_model_func': <function openrouter_openai_complete at 0x79ef71fc6ac0>, 'llm_model_name': 'deepseek/deepseek-chat-v3-0324:free', 'llm_model_max_token_size': 200, 'llm_model_max_async': 16, 'llm_model_kwargs': {}, 'vector_db_storage_cls_kwargs': {'cosine_better_than_threshold': '0.2'}, 'enable_llm_cache': True, 'addon_params': {}, 'convert_response_to_json_func': <function convert_response_to_json at 0x79f04b657600>, 'doc_status_storage': 'PGDocStatusStorage', 'chunking_func': <function chunking_by_token_size at 0x79f0eb7f7b00>, 'chunking_func_kwargs': {}, 'max_parallel_insert': 2}, embedding_func=<function limit_async_func_call.<locals>.final_decro.<locals>.wait_func at 0x79ef73377b00>, meta_fields={'entity_name'}, cosine_better_than_threshold='0.2', db=<minirag.kg.postgres_impl.PostgreSQLDB object at 0x79ef72226150>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag.entities_vdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8989cf6d-e64b-439b-9f59-96b820ec9b73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "functools.partial(<function lazy_external_import.<locals>.import_class at 0x79ef81d4b060>, global_config={'working_dir': '/tmp/minirag_demo', 'kv_storage': 'PGKVStorage', 'vector_storage': 'PGVectorStorage', 'graph_storage': 'PGGraphStorage', 'log_level': 0, 'chunk_token_size': 1200, 'chunk_overlap_token_size': 100, 'tiktoken_model_name': 'gpt-4o-mini', 'entity_extract_max_gleaning': 1, 'entity_summary_to_max_tokens': 500, 'node_embedding_algorithm': 'node2vec', 'node2vec_params': {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3}, 'embedding_func': {'embedding_dim': 384, 'max_token_size': 1000, 'func': <function setup_rag_system.<locals>.<lambda> at 0x79ef71c3dee0>}, 'embedding_batch_num': 32, 'embedding_func_max_async': 16, 'llm_model_func': <function openrouter_openai_complete at 0x79ef71fc6ac0>, 'llm_model_name': 'deepseek/deepseek-chat-v3-0324:free', 'llm_model_max_token_size': 200, 'llm_model_max_async': 16, 'llm_model_kwargs': {}, 'vector_db_storage_cls_kwargs': {'cosine_better_than_threshold': '0.2'}, 'enable_llm_cache': True, 'addon_params': {}, 'convert_response_to_json_func': <function convert_response_to_json at 0x79f04b657600>, 'doc_status_storage': 'PGDocStatusStorage', 'chunking_func': <function chunking_by_token_size at 0x79f0eb7f7b00>, 'chunking_func_kwargs': {}, 'max_parallel_insert': 2})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag.vector_db_storage_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "26a073e0-ab0f-4677-8f4e-a195b31d9e16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "functools.partial(<function lazy_external_import.<locals>.import_class at 0x79ef73fe14e0>, global_config={'working_dir': '/tmp/minirag_demo', 'kv_storage': 'PGKVStorage', 'vector_storage': 'PGVectorStorage', 'graph_storage': 'PGGraphStorage', 'log_level': 0, 'chunk_token_size': 1200, 'chunk_overlap_token_size': 100, 'tiktoken_model_name': 'gpt-4o-mini', 'entity_extract_max_gleaning': 1, 'entity_summary_to_max_tokens': 500, 'node_embedding_algorithm': 'node2vec', 'node2vec_params': {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3}, 'embedding_func': {'embedding_dim': 384, 'max_token_size': 1000, 'func': <function setup_rag_system.<locals>.<lambda> at 0x79ef71c3dee0>}, 'embedding_batch_num': 32, 'embedding_func_max_async': 16, 'llm_model_func': <function openrouter_openai_complete at 0x79ef71fc6ac0>, 'llm_model_name': 'deepseek/deepseek-chat-v3-0324:free', 'llm_model_max_token_size': 200, 'llm_model_max_async': 16, 'llm_model_kwargs': {}, 'vector_db_storage_cls_kwargs': {'cosine_better_than_threshold': '0.2'}, 'enable_llm_cache': True, 'addon_params': {}, 'convert_response_to_json_func': <function convert_response_to_json at 0x79f04b657600>, 'doc_status_storage': 'PGDocStatusStorage', 'chunking_func': <function chunking_by_token_size at 0x79f0eb7f7b00>, 'chunking_func_kwargs': {}, 'max_parallel_insert': 2})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag.graph_storage_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "95c80c16-11c9-4a4a-ae7e-c0bcb5c201ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PGDocStatusStorage(namespace='doc_status', global_config={'working_dir': '/tmp/minirag_demo', 'kv_storage': 'PGKVStorage', 'vector_storage': 'PGVectorStorage', 'graph_storage': 'PGGraphStorage', 'log_level': 0, 'chunk_token_size': 1200, 'chunk_overlap_token_size': 100, 'tiktoken_model_name': 'gpt-4o-mini', 'entity_extract_max_gleaning': 1, 'entity_summary_to_max_tokens': 500, 'node_embedding_algorithm': 'node2vec', 'node2vec_params': {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3}, 'embedding_func': <function limit_async_func_call.<locals>.final_decro.<locals>.wait_func at 0x79ef73377b00>, 'embedding_batch_num': 32, 'embedding_func_max_async': 16, 'llm_model_func': <function openrouter_openai_complete at 0x79ef71fc6ac0>, 'llm_model_name': 'deepseek/deepseek-chat-v3-0324:free', 'llm_model_max_token_size': 200, 'llm_model_max_async': 16, 'llm_model_kwargs': {}, 'vector_db_storage_cls_kwargs': {'cosine_better_than_threshold': '0.2'}, 'enable_llm_cache': True, 'addon_params': {}, 'convert_response_to_json_func': <function convert_response_to_json at 0x79f04b657600>, 'doc_status_storage': 'PGDocStatusStorage', 'chunking_func': <function chunking_by_token_size at 0x79f0eb7f7b00>, 'chunking_func_kwargs': {}, 'max_parallel_insert': 2}, embedding_func=None, db=<minirag.kg.postgres_impl.PostgreSQLDB object at 0x79ef72226150>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag.doc_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b0c9bb29-75c3-4d0a-89c3-421be38d0485",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PGVectorStorage(namespace='chunks', global_config={'working_dir': '/tmp/minirag_demo', 'kv_storage': 'PGKVStorage', 'vector_storage': 'PGVectorStorage', 'graph_storage': 'PGGraphStorage', 'log_level': 0, 'chunk_token_size': 1200, 'chunk_overlap_token_size': 100, 'tiktoken_model_name': 'gpt-4o-mini', 'entity_extract_max_gleaning': 1, 'entity_summary_to_max_tokens': 500, 'node_embedding_algorithm': 'node2vec', 'node2vec_params': {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3}, 'embedding_func': <function limit_async_func_call.<locals>.final_decro.<locals>.wait_func at 0x79ef73377b00>, 'embedding_batch_num': 32, 'embedding_func_max_async': 16, 'llm_model_func': <function openrouter_openai_complete at 0x79ef71fc6ac0>, 'llm_model_name': 'deepseek/deepseek-chat-v3-0324:free', 'llm_model_max_token_size': 200, 'llm_model_max_async': 16, 'llm_model_kwargs': {}, 'vector_db_storage_cls_kwargs': {'cosine_better_than_threshold': '0.2'}, 'enable_llm_cache': True, 'addon_params': {}, 'convert_response_to_json_func': <function convert_response_to_json at 0x79f04b657600>, 'doc_status_storage': 'PGDocStatusStorage', 'chunking_func': <function chunking_by_token_size at 0x79f0eb7f7b00>, 'chunking_func_kwargs': {}, 'max_parallel_insert': 2}, embedding_func=<function limit_async_func_call.<locals>.final_decro.<locals>.wait_func at 0x79ef73377b00>, meta_fields=set(), cosine_better_than_threshold='0.2', db=<minirag.kg.postgres_impl.PostgreSQLDB object at 0x79ef72226150>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag.chunks_vdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dddf43f6-e428-49c4-851d-ee2a8e9a430b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PGVectorStorage(namespace='relationships', global_config={'working_dir': '/tmp/minirag_demo', 'kv_storage': 'PGKVStorage', 'vector_storage': 'PGVectorStorage', 'graph_storage': 'PGGraphStorage', 'log_level': 0, 'chunk_token_size': 1200, 'chunk_overlap_token_size': 100, 'tiktoken_model_name': 'gpt-4o-mini', 'entity_extract_max_gleaning': 1, 'entity_summary_to_max_tokens': 500, 'node_embedding_algorithm': 'node2vec', 'node2vec_params': {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3}, 'embedding_func': <function limit_async_func_call.<locals>.final_decro.<locals>.wait_func at 0x79ef73377b00>, 'embedding_batch_num': 32, 'embedding_func_max_async': 16, 'llm_model_func': <function openrouter_openai_complete at 0x79ef71fc6ac0>, 'llm_model_name': 'deepseek/deepseek-chat-v3-0324:free', 'llm_model_max_token_size': 200, 'llm_model_max_async': 16, 'llm_model_kwargs': {}, 'vector_db_storage_cls_kwargs': {'cosine_better_than_threshold': '0.2'}, 'enable_llm_cache': True, 'addon_params': {}, 'convert_response_to_json_func': <function convert_response_to_json at 0x79f04b657600>, 'doc_status_storage': 'PGDocStatusStorage', 'chunking_func': <function chunking_by_token_size at 0x79f0eb7f7b00>, 'chunking_func_kwargs': {}, 'max_parallel_insert': 2}, embedding_func=<function limit_async_func_call.<locals>.final_decro.<locals>.wait_func at 0x79ef73377b00>, meta_fields={'tgt_id', 'src_id'}, cosine_better_than_threshold='0.2', db=<minirag.kg.postgres_impl.PostgreSQLDB object at 0x79ef72226150>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag.relationships_vdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67412973-3239-4d19-8d9e-fb4739eecda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag.entities_vdb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7593e7-b03a-41aa-8846-33e5361fee0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a5ab44-873d-4507-a9f3-88c1d1e1a821",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608116b7-7da0-477d-9f4a-70c26c26370c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cad0be1-f8b3-4b87-ac8f-1867bbbd7c64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9038fe2-03c7-43cd-aa0b-f3735e2009bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad2fa755-b2e1-4b5f-b12e-1e914b051068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minirag.kg.postgres_impl.PostgreSQLDB at 0x79ef72226150>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    # def set_storage_client(self, db_client):\n",
    "    #     # Now only tested on Oracle Database\n",
    "    #     for storage in [\n",
    "    #         self.vector_db_storage_cls,\n",
    "    #         self.graph_storage_cls,\n",
    "    #         self.doc_status,\n",
    "    #         self.full_docs,\n",
    "    #         self.text_chunks,\n",
    "    #         self.llm_response_cache,\n",
    "    #         self.key_string_value_json_storage_cls,\n",
    "    #         self.chunks_vdb,\n",
    "    #         self.relationships_vdb,\n",
    "    #         self.entities_vdb,\n",
    "    #         self.graph_storage_cls,\n",
    "    #         self.chunk_entity_relation_graph,\n",
    "    #         self.llm_response_cache,\n",
    "    #     ]:\n",
    "    #         # set client\n",
    "    #         storage.db = db_client\n",
    "\n",
    "\n",
    "rag.entities_vdb.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c654c95d-9a9d-4f9a-8cb3-ee37847bbe71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minirag.kg.postgres_impl.PostgreSQLDB at 0x79ef72226150>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PGVectorStorage\n",
    "rag.vector_db_storage_cls.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcc5489-835d-4ff6-9cdf-247611f6adce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1daea069-cb7f-4fc6-9452-01b795bc41d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce6594c-f98e-4f59-aafc-e56a45cd55c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24977ecf-f0db-4c96-8504-5e93a8cac5a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966d465b-918d-4cd3-8fa0-81c865566023",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba35b46-c332-4034-9cd5-80e00154a64c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff57561-822b-4dd8-b9a4-ce05ef7ed0e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeed8968-b933-452f-9cf2-825838b79052",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec781ff1-f4c1-4dbb-9666-173510b15a87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f2b3c6-3916-4a0e-8d73-86e005552b4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13314576-7ba0-401e-9a2a-76e5a6843745",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ad6d5e-9382-4dc3-bf98-95d3b64e4cc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d308a228-27bf-40a3-af76-e9c22a076771",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 変更結果を画面に表示して確認する\n",
    "# !sed 's/where workspace=$1 and status=$1/where workspace=$1 and status=$2/' minirag/kg/postgres_impl.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fded19-1aeb-4fbc-a405-6705f5bd651e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e89445-1a03-4bf3-8cd1-aeb2f6278839",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b210a9df-43fc-4746-9882-092cd041ff54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'postgresql://postgres_user:postgres_pass@postgres:5432/my_database'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getenv(\"DATABASE_URL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fe773e2e-57b4-4b6f-9de0-e6424482005f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:minirag:Using the label default for PostgreSQL as identifier\n",
      "INFO:minirag:Connected to PostgreSQL database at postgres:5432/my_database\n",
      "ERROR:minirag:PostgreSQL database error: relation \"lightrag_doc_full\" does not exist\n",
      "ERROR:minirag:Failed to check table LIGHTRAG_DOC_FULL in PostgreSQL database\n",
      "ERROR:minirag:PostgreSQL database error: relation \"lightrag_doc_full\" does not exist\n",
      "INFO:minirag:Created table LIGHTRAG_DOC_FULL in PostgreSQL database\n",
      "ERROR:minirag:PostgreSQL database error: relation \"lightrag_doc_chunks\" does not exist\n",
      "ERROR:minirag:Failed to check table LIGHTRAG_DOC_CHUNKS in PostgreSQL database\n",
      "ERROR:minirag:PostgreSQL database error: relation \"lightrag_doc_chunks\" does not exist\n",
      "INFO:minirag:Created table LIGHTRAG_DOC_CHUNKS in PostgreSQL database\n",
      "ERROR:minirag:PostgreSQL database error: relation \"lightrag_vdb_entity\" does not exist\n",
      "ERROR:minirag:Failed to check table LIGHTRAG_VDB_ENTITY in PostgreSQL database\n",
      "ERROR:minirag:PostgreSQL database error: relation \"lightrag_vdb_entity\" does not exist\n",
      "INFO:minirag:Created table LIGHTRAG_VDB_ENTITY in PostgreSQL database\n",
      "ERROR:minirag:PostgreSQL database error: relation \"lightrag_vdb_relation\" does not exist\n",
      "ERROR:minirag:Failed to check table LIGHTRAG_VDB_RELATION in PostgreSQL database\n",
      "ERROR:minirag:PostgreSQL database error: relation \"lightrag_vdb_relation\" does not exist\n",
      "INFO:minirag:Created table LIGHTRAG_VDB_RELATION in PostgreSQL database\n",
      "ERROR:minirag:PostgreSQL database error: relation \"lightrag_llm_cache\" does not exist\n",
      "ERROR:minirag:Failed to check table LIGHTRAG_LLM_CACHE in PostgreSQL database\n",
      "ERROR:minirag:PostgreSQL database error: relation \"lightrag_llm_cache\" does not exist\n",
      "INFO:minirag:Created table LIGHTRAG_LLM_CACHE in PostgreSQL database\n",
      "ERROR:minirag:PostgreSQL database error: relation \"lightrag_doc_status\" does not exist\n",
      "ERROR:minirag:Failed to check table LIGHTRAG_DOC_STATUS in PostgreSQL database\n",
      "ERROR:minirag:PostgreSQL database error: relation \"lightrag_doc_status\" does not exist\n",
      "INFO:minirag:Created table LIGHTRAG_DOC_STATUS in PostgreSQL database\n",
      "INFO:minirag:Finished checking all tables in PostgreSQL database\n",
      "INFO:minirag:Logger initialized for working directory: /tmp/minirag_demo\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "データベースに接続しています...\n",
      "データベースに接続しました！\n",
      "テーブルの存在を確認・作成しています...\n",
      "SELECT 1 FROM LIGHTRAG_DOC_FULL LIMIT 1\n",
      "None\n",
      "SELECT 1 FROM LIGHTRAG_DOC_CHUNKS LIMIT 1\n",
      "None\n",
      "SELECT 1 FROM LIGHTRAG_VDB_ENTITY LIMIT 1\n",
      "None\n",
      "SELECT 1 FROM LIGHTRAG_VDB_RELATION LIMIT 1\n",
      "None\n",
      "SELECT 1 FROM LIGHTRAG_LLM_CACHE LIMIT 1\n",
      "None\n",
      "SELECT 1 FROM LIGHTRAG_DOC_STATUS LIMIT 1\n",
      "None\n",
      "テーブルの準備が完了しました！\n",
      "------------------------- MiniRAGが初期化されました！ -------------------------\n"
     ]
    }
   ],
   "source": [
    "async def openrouter_openai_complete(\n",
    "    prompt,\n",
    "    system_prompt=None,\n",
    "    history_messages=[],\n",
    "    keyword_extraction=False,\n",
    "    api_key: str = None,\n",
    "    **kwargs,\n",
    ") -> str:\n",
    "    # if api_key:\n",
    "    #     os.environ[\"OPENROUTER_API_KEY\"] = api_key\n",
    "\n",
    "    keyword_extraction = kwargs.pop(\"keyword_extraction\", None)\n",
    "    result = await openai_complete_if_cache(\n",
    "        LLM_MODEL,  # change accordingly\n",
    "        prompt,\n",
    "        system_prompt=system_prompt,\n",
    "        history_messages=history_messages,\n",
    "        base_url=\"https://openrouter.ai/api/v1\",\n",
    "        api_key=api_key,\n",
    "        **kwargs,\n",
    "    )\n",
    "    if keyword_extraction:  # TODO: use JSON API\n",
    "        return locate_json_string_body_from_string(result)\n",
    "    return result\n",
    "\n",
    "async def setup_rag_system():\n",
    "    \"\"\"RAGシステムを初期化し、準備ができたインスタンスを返す\"\"\"\n",
    "\n",
    "    # 1. データベース設定\n",
    "    db_config={\n",
    "        \"host\": \"postgres\",    # これは Docker のサービス名「postgres」で指定\n",
    "        \"port\": 5432,\n",
    "        \"user\": os.getenv(\"POSTGRES_USER\"),\n",
    "        \"password\": os.getenv(\"POSTGRES_PASSWORD\"),\n",
    "        \"database\": os.getenv(\"POSTGRES_DB\"),\n",
    "    }\n",
    "\n",
    "    # 2. PostgreSQLDBインスタンスを作成\n",
    "    db_postgre = PostgreSQLDB(config=db_config)\n",
    "\n",
    "    # 3. データベース接続を初期化\n",
    "    print(\"データベースに接続しています...\")\n",
    "    await db_postgre.initdb()\n",
    "    print(\"データベースに接続しました！\")\n",
    "\n",
    "    # 必要なテーブルが存在するかチェックし、なければ作成する\n",
    "    print(\"テーブルの存在を確認・作成しています...\")\n",
    "    await db_postgre.check_tables()\n",
    "    print(\"テーブルの準備が完了しました！\")\n",
    "\n",
    "    # 5. MiniRAGインスタンスを作成\n",
    "    os.environ[\"AGE_GRAPH_NAME\"] = \"my_minirag_graph\" # または設定から取得します\n",
    "    rag = MiniRAG(\n",
    "        working_dir=WORKING_DIR,\n",
    "    \n",
    "        # ポスグレ\n",
    "        # kv_storage=\"PGKVStorage\",\n",
    "        # vector_storage=\"PGVectorStorage\",\n",
    "        # graph_storage=\"PGGraphStorage\",\n",
    "    \n",
    "        # llm_model_func=hf_model_complete,\n",
    "        # llm_model_func=gemini_2_5_flash_complete,\n",
    "        llm_model_func=openrouter_openai_complete,\n",
    "    \n",
    "        llm_model_max_token_size=200,\n",
    "        llm_model_name=LLM_MODEL,\n",
    "        embedding_func=EmbeddingFunc(\n",
    "            embedding_dim=EMBEDDING_DIM,\n",
    "            max_token_size=1000,\n",
    "            func=lambda texts: hf_embed(\n",
    "                texts,\n",
    "                tokenizer=AutoTokenizer.from_pretrained(EMBEDDING_MODEL),\n",
    "                embed_model=AutoModel.from_pretrained(EMBEDDING_MODEL),\n",
    "            ),\n",
    "        ),\n",
    "        kv_storage=\"PGKVStorage\",\n",
    "        vector_storage=\"PGVectorStorage\",\n",
    "        graph_storage=\"PGGraphStorage\",\n",
    "        doc_status_storage=\"PGDocStatusStorage\",\n",
    "        vector_db_storage_cls_kwargs={\n",
    "            \"cosine_better_than_threshold\": os.getenv(\"COSINE_THRESHOLD\")\n",
    "        }\n",
    "    )\n",
    "    rag.set_storage_client(db_postgre)    \n",
    "    return rag\n",
    "\n",
    "# RAGシステムをセットアップ\n",
    "try:\n",
    "    rag = await setup_rag_system()\n",
    "    print(\"------------------------- MiniRAGが初期化されました！ -------------------------\")\n",
    "except Exception as e:\n",
    "    print(f\"RAGシステムのセットアップに失敗しました: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9294d8b7-6899-4060-ae2e-c6b71a81f531",
   "metadata": {},
   "outputs": [],
   "source": [
    "INFO:minirag:Using the label default for PostgreSQL as identifier\n",
    "INFO:minirag:Connected to PostgreSQL database at postgres:5432/my_database\n",
    "ERROR:minirag:PostgreSQL database error: relation \"lightrag_doc_full\" does not exist\n",
    "ERROR:minirag:Failed to check table LIGHTRAG_DOC_FULL in PostgreSQL database\n",
    "ERROR:minirag:PostgreSQL database error: relation \"lightrag_doc_full\" does not exist\n",
    "INFO:minirag:Created table LIGHTRAG_DOC_FULL in PostgreSQL database\n",
    "ERROR:minirag:PostgreSQL database error: relation \"lightrag_doc_chunks\" does not exist\n",
    "ERROR:minirag:Failed to check table LIGHTRAG_DOC_CHUNKS in PostgreSQL database\n",
    "ERROR:minirag:PostgreSQL database error: relation \"lightrag_doc_chunks\" does not exist\n",
    "INFO:minirag:Created table LIGHTRAG_DOC_CHUNKS in PostgreSQL database\n",
    "ERROR:minirag:PostgreSQL database error: relation \"lightrag_vdb_entity\" does not exist\n",
    "ERROR:minirag:Failed to check table LIGHTRAG_VDB_ENTITY in PostgreSQL database\n",
    "ERROR:minirag:PostgreSQL database error: relation \"lightrag_vdb_entity\" does not exist\n",
    "INFO:minirag:Created table LIGHTRAG_VDB_ENTITY in PostgreSQL database\n",
    "ERROR:minirag:PostgreSQL database error: relation \"lightrag_vdb_relation\" does not exist\n",
    "ERROR:minirag:Failed to check table LIGHTRAG_VDB_RELATION in PostgreSQL database\n",
    "ERROR:minirag:PostgreSQL database error: relation \"lightrag_vdb_relation\" does not exist\n",
    "INFO:minirag:Created table LIGHTRAG_VDB_RELATION in PostgreSQL database\n",
    "ERROR:minirag:PostgreSQL database error: relation \"lightrag_llm_cache\" does not exist\n",
    "ERROR:minirag:Failed to check table LIGHTRAG_LLM_CACHE in PostgreSQL database\n",
    "ERROR:minirag:PostgreSQL database error: relation \"lightrag_llm_cache\" does not exist\n",
    "INFO:minirag:Created table LIGHTRAG_LLM_CACHE in PostgreSQL database\n",
    "ERROR:minirag:PostgreSQL database error: relation \"lightrag_doc_status\" does not exist\n",
    "ERROR:minirag:Failed to check table LIGHTRAG_DOC_STATUS in PostgreSQL database\n",
    "ERROR:minirag:PostgreSQL database error: relation \"lightrag_doc_status\" does not exist\n",
    "INFO:minirag:Created table LIGHTRAG_DOC_STATUS in PostgreSQL database\n",
    "INFO:minirag:Finished checking all tables in PostgreSQL database\n",
    "INFO:minirag:Logger initialized for working directory: /tmp/minirag_demo\n",
    "\n",
    "上記の表示されている ERROR ログは、一見すると問題のように見えますが、実際には 「テーブルが存在しないことを検知した」 という正常な動作の一部です。check_tables メソッドは、このエラーを意図的に発生させて、テーブルが存在しない場合にのみ作成処理を行うように設計されています。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe42312b-e479-471c-a275-7116a4f0d004",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba06a53a-f8f6-4c28-a205-9973dff1d0a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210b99ea-ba92-4bc1-9669-1f3efaa14d9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f294b3c1-0c01-4339-a059-928efebba9f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

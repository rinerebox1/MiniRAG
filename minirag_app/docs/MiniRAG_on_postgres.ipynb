{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6875c5de-f21a-4610-a8fc-ef219ee61dc0",
   "metadata": {},
   "source": [
    "# MiniRAG を postgres で実行する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9399e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要なライブラリのインポート\n",
    "import os\n",
    "import tempfile\n",
    "from minirag import MiniRAG, QueryParam\n",
    "from minirag.llm.hf import (\n",
    "    hf_model_complete,\n",
    "    hf_embed,\n",
    ")\n",
    "# from minirag.llm.openai import openrouter_openai_complete\n",
    "from minirag.llm.openai import openai_complete_if_cache\n",
    "from minirag.utils import EmbeddingFunc\n",
    "from minirag.utils import (\n",
    "    wrap_embedding_func_with_attrs,\n",
    "    locate_json_string_body_from_string,\n",
    "    safe_unicode_decode,\n",
    "    logger,\n",
    ")\n",
    "\n",
    "import asyncpg\n",
    "from psycopg_pool import AsyncConnectionPool\n",
    "from minirag.kg.postgres_impl import PostgreSQLDB\n",
    "from minirag.kg.postgres_impl import PGKVStorage\n",
    "from minirag.kg.postgres_impl import PGVectorStorage\n",
    "from minirag.kg.postgres_impl import PGGraphStorage\n",
    "from minirag.kg.postgres_impl import PGDocStatusStorage\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from tenacity import retry, stop_after_attempt, wait_random_exponential, retry_if_exception\n",
    "from openai import RateLimitError, APIStatusError # openaiライブラリが投げる例外をインポート\n",
    "import asyncio\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a8f379-c151-472f-9720-3d9e063628fe",
   "metadata": {},
   "source": [
    "### データベース接続テスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "913d86fc-9456-4a42-baf5-cbcf7ec23318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'version': 'PostgreSQL 16.9 (Debian 16.9-1.pgdg120+1) on x86_64-pc-linux-gnu, compiled by gcc (Debian 12.2.0-14) 12.2.0, 64-bit'}\n"
     ]
    }
   ],
   "source": [
    "import psycopg\n",
    "from psycopg.rows import dict_row\n",
    "\n",
    "def get_conn():\n",
    "    url = os.getenv(\"DATABASE_URL\")\n",
    "    if url:\n",
    "        return psycopg.connect(url, row_factory=dict_row)\n",
    "    # fallback: assemble from separate vars\n",
    "    dsn = (\n",
    "        f\"host={os.getenv('PGHOST','db')} \"\n",
    "        f\"port={os.getenv('PGPORT','5432')} \"\n",
    "        f\"dbname={os.getenv('PGDATABASE','postgres')} \"\n",
    "        f\"user={os.getenv('POSTGRES_USER','postgres')} \"\n",
    "        f\"password={os.getenv('POSTGRES_PASSWORD')}\"\n",
    "    )\n",
    "    return psycopg.connect(dsn, row_factory=dict_row)\n",
    "\n",
    "with get_conn() as conn, conn.cursor() as cur:\n",
    "    cur.execute(\"SELECT version();\")\n",
    "    print(cur.fetchone())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f907bca4-e72e-4308-a6f3-2872d0324002",
   "metadata": {},
   "source": [
    "### ハイブリッドクエリのテスト\n",
    "- 前提: 先に init_db.sh が実行されテーブルが作成されていること"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9a51793-190e-4933-b145-adbc7c90679e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "データベースに接続し、クエリを実行します...\n",
      "\n",
      "--- クエリ実行結果 ---\n",
      "{'id': 1, 'name': '商品A', 'embedding': '[0.1,0.2,0.3]'}\n",
      "{'id': 2, 'name': '商品B', 'embedding': '[0.2,0.1,0.9]'}\n"
     ]
    }
   ],
   "source": [
    "def get_conn():\n",
    "    \"\"\"\n",
    "    環境変数からデータベース接続情報を取得し、接続オブジェクトを返す。\n",
    "    DATABASE_URLが設定されていればそれを使用し、なければ個別の変数を組み立てる。\n",
    "    結果は辞書形式で返されるように設定済み。\n",
    "    \"\"\"\n",
    "    url = os.getenv(\"DATABASE_URL\")\n",
    "    if url:\n",
    "        # DATABASE_URLが設定されている場合\n",
    "        return psycopg.connect(url, row_factory=dict_row)\n",
    "    \n",
    "    # DATABASE_URLがない場合、個別の環境変数からDSNを組み立てる\n",
    "    # PGPASSWORDが設定されていないと接続に失敗するため、チェックを追加するとより親切\n",
    "    password = os.getenv('PGPASSWORD')\n",
    "    if not password:\n",
    "        raise ValueError(\"環境変数 PGPASSWORD が設定されていません。\")\n",
    "        \n",
    "    dsn = (\n",
    "        f\"host={os.getenv('PGHOST', 'localhost')} \"\n",
    "        f\"port={os.getenv('PGPORT', '5432')} \"\n",
    "        f\"dbname={os.getenv('PGDATABASE', 'postgres')} \"\n",
    "        f\"user={os.getenv('POSTGRES_USER', 'postgres')} \"\n",
    "        f\"password={password}\"\n",
    "    )\n",
    "    return psycopg.connect(dsn, row_factory=dict_row)\n",
    "\n",
    "def find_recommended_products_for_alice(_results):\n",
    "    \"\"\"\n",
    "    指定されたSQLクエリを実行し、'Alice'が好む商品とベクトル的に類似した商品を取得する。\n",
    "    \"\"\"\n",
    "    # 実行したいSQLクエリを三重クォートで定義\n",
    "    # これにより、複数行のクエリを読みやすく記述できる\n",
    "    sql_query = \"\"\"\n",
    "    SELECT\n",
    "        p.id,\n",
    "        p.name,\n",
    "        p.embedding\n",
    "    FROM\n",
    "        public.products AS p\n",
    "    JOIN\n",
    "        -- cypher関数でグラフクエリを実行し、結果をテーブルのように扱う\n",
    "        cypher('my_minirag_graph', $$\n",
    "            MATCH (u:User {name: 'Alice'})-[:LIKES]->(prod:Product)\n",
    "            RETURN prod.product_id\n",
    "        $$) AS liked(product_id agtype)\n",
    "    ON\n",
    "        -- グラフクエリの結果(agtype)を整数にキャストしてproductsテーブルのIDと結合\n",
    "        p.id = (liked.product_id)::INTEGER\n",
    "    ORDER BY\n",
    "        -- pgvectorの<->演算子で、指定ベクトルとのコサイン距離が近い順に並び替え\n",
    "        p.embedding <=> '[0.1, 0.1, 0.2]';\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # with文で接続とカーソルを管理し、処理終了後に自動でクローズする\n",
    "        with get_conn() as conn, conn.cursor() as cur:\n",
    "            print(\"データベースに接続し、クエリを実行します...\")\n",
    "            \n",
    "            # クエリの実行\n",
    "            cur.execute(sql_query)\n",
    "            \n",
    "            # 全ての実行結果を取得 (fetchall)\n",
    "            # 1件だけなら fetchone(), 複数件なら fetchall() を使う\n",
    "            results = cur.fetchall()\n",
    "            \n",
    "            print(\"\\n--- クエリ実行結果 ---\")\n",
    "            if results:\n",
    "                # 取得した結果を1行ずつ表示\n",
    "                for row in results:\n",
    "                    print(row)\n",
    "                    _results.append(row)\n",
    "            else:\n",
    "                print(\"条件に一致する結果は見つかりませんでした。\")\n",
    "        return _results\n",
    "\n",
    "    except psycopg.Error as e:\n",
    "        print(f\"データベースエラーが発生しました: {e}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"設定エラーが発生しました: {e}\")\n",
    "\n",
    "query_results = []\n",
    "query_results = find_recommended_products_for_alice(query_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211ca750-bd82-43c8-9d52-421afb4771ed",
   "metadata": {},
   "source": [
    "### マークダウンに整形"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b391635-b718-4889-a334-52ab7e15fb45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| id | name | embedding |\n",
      "| --- | --- | --- |\n",
      "| 1 | 商品A | [0.1,0.2,0.3] |\n",
      "| 2 | 商品B | [0.2,0.1,0.9] |\n"
     ]
    }
   ],
   "source": [
    "def convert_to_markdown(data: list[dict]) -> str:\n",
    "    \"\"\"\n",
    "    辞書のリストをマークダウンのテーブル形式に変換します。\n",
    "\n",
    "    Args:\n",
    "        data: 辞書のリスト。各辞書がテーブルの1行に対応します。\n",
    "\n",
    "    Returns:\n",
    "        マークダウン形式のテーブル文字列。\n",
    "    \"\"\"\n",
    "    if not data:\n",
    "        return \"データがありません。\"\n",
    "\n",
    "    # ヘッダーの作成 (最初のデータのキーから取得)\n",
    "    headers = data[0].keys()\n",
    "    header_line = \"| \" + \" | \".join(headers) + \" |\"\n",
    "\n",
    "    # 区切り線の作成\n",
    "    separator_line = \"| \" + \" | \".join([\"---\"] * len(headers)) + \" |\"\n",
    "\n",
    "    # データ行の作成\n",
    "    data_lines = []\n",
    "    for row in data:\n",
    "        # 各値を文字列に変換して結合\n",
    "        values = [str(row.get(h, \"\")) for h in headers]\n",
    "        data_lines.append(\"| \" + \" | \".join(values) + \" |\")\n",
    "\n",
    "    # 全ての行を結合して返す\n",
    "    return \"\\n\".join([header_line, separator_line] + data_lines)\n",
    "\n",
    "# --- 変換を実行して結果を表示 ---\n",
    "markdown_table = convert_to_markdown(query_results)\n",
    "print(markdown_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ece4d57a-b30d-4275-b0ca-9a8e5435e645",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  id | name  |   embedding   \n",
    "# ----+-------+---------------\n",
    "#   1 | 商品A | [0.1,0.2,0.3]\n",
    "#   2 | 商品B | [0.2,0.1,0.9]\n",
    "\n",
    "# 上記の結果になればOK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d941081d-405c-4833-86fa-f5bcdd0b3c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Openrouter APIキーが設定されました\n",
      "\n",
      "HuggingFace TOKEN が設定されました\n"
     ]
    }
   ],
   "source": [
    "# API KEY が見えないようにコメントアウト\n",
    "\n",
    "# print(os.getenv(\"OPENROUTER_API_KEY\"))\n",
    "# print(os.getenv(\"OPENAI_API_KEY\"))\n",
    "print(\"Openrouter APIキーが設定されました\")\n",
    "print()\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "# print(HF_TOKEN)\n",
    "print(\"HuggingFace TOKEN が設定されました\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c16a765-6d23-439b-a433-e40df1cb977d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "作業ディレクトリ: /tmp/minirag_demo\n"
     ]
    }
   ],
   "source": [
    "use_japanese_embedding = False\n",
    "\n",
    "if use_japanese_embedding:\n",
    "    # MINI モードでの回答ができなくなったので、逆に精度が落ちたかも。\n",
    "    EMBEDDING_MODEL = \"hotchpotch/static-embedding-japanese\"\n",
    "    EMBEDDING_DIM   = 1024\n",
    "    TOKENIZER_MODEL = \"hotchpotch/xlm-roberta-japanese-tokenizer\"\n",
    "    print(\"-------------- static-embedding-japanese を使用します --------------\")\n",
    "else:\n",
    "    # 埋め込みモデルの設定\n",
    "    EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    EMBEDDING_DIM   = 384\n",
    "    print(\"-------------- all-MiniLM-L6-v2 を使用します --------------\")\n",
    "\n",
    "\n",
    "# LLMの設定\n",
    "# LLM_MODEL = \"Qwen/Qwen3-1.7B\"  # または \"Qwen/Qwen3-4B\", \"Qwen/Qwen3-1.7B\" など\n",
    "# LLM_MODEL = \"jaeyong2/Qwen2.5-3B-Instruct-Ja-SFT\"\n",
    "LLM_MODEL = \"deepseek/deepseek-chat-v3-0324:free\"\n",
    "\n",
    "\n",
    "\n",
    "# 作業ディレクトリの作成\n",
    "WORKING_DIR = \"/tmp/minirag_demo\"\n",
    "os.makedirs(WORKING_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"作業ディレクトリ: {WORKING_DIR}\")\n",
    "\n",
    "\n",
    "# DATA_PATH = args.datapath\n",
    "# QUERY_PATH = args.querypath\n",
    "# OUTPUT_PATH = args.outputpath\n",
    "# print(\"USING LLM:\", LLM_MODEL)\n",
    "# print(\"USING WORKING DIR:\", WORKING_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30f93e21-c206-4407-8f94-8e670dc30fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_japanese_embedding:\n",
    "\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    model = SentenceTransformer(EMBEDDING_MODEL, device=\"cpu\", token=HF_TOKEN)\n",
    "    \n",
    "    query = \"美味しいラーメン屋に行きたい\"\n",
    "    docs = [\n",
    "        \"素敵なカフェが近所にあるよ。落ち着いた雰囲気でゆっくりできるし、窓際の席からは公園の景色も見えるんだ。\",\n",
    "        \"新鮮な魚介を提供する店です。地元の漁師から直接仕入れているので鮮度は抜群ですし、料理人の腕も確かです。\",\n",
    "        \"あそこは行きにくいけど、隠れた豚骨の名店だよ。スープが最高だし、麺の硬さも好み。\",\n",
    "        \"おすすめの中華そばの店を教えてあげる。とりわけチャーシューが手作りで柔らかくてジューシーなんだ。\",\n",
    "    ]\n",
    "    \n",
    "    embeddings = model.encode([query] + docs)\n",
    "    print(embeddings.shape)\n",
    "    similarities = model.similarity(embeddings[0], embeddings[1:])\n",
    "    for i, similarity in enumerate(similarities[0].tolist()):\n",
    "        print(f\"{similarity:.04f}: {docs[i]}\")\n",
    "\n",
    "else:\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL, token=HF_TOKEN)\n",
    "    model = AutoModel.from_pretrained(EMBEDDING_MODEL, token=HF_TOKEN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00688add-fcf5-4517-85c1-2eae3ae5f696",
   "metadata": {},
   "source": [
    "## transformer の API で使えるように変換する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87149d98-7010-4b7d-96c1-b0b2566fa72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_japanese_embedding:\n",
    "    \n",
    "    import torch\n",
    "    from torch import nn\n",
    "    from transformers import PreTrainedModel, PretrainedConfig\n",
    "    from transformers.modeling_outputs import BaseModelOutputWithPoolingAndCrossAttentions\n",
    "    \n",
    "    \n",
    "    class StaticEmbeddingConfig(PretrainedConfig):\n",
    "        model_type = \"static-embedding\"\n",
    "    \n",
    "        def __init__(self, vocab_size=32768, hidden_size=1024, pad_token_id=0, **kwargs):\n",
    "            super().__init__(pad_token_id=pad_token_id, **kwargs)\n",
    "            self.vocab_size = vocab_size\n",
    "            self.hidden_size = hidden_size\n",
    "    \n",
    "    \n",
    "    class StaticEmbeddingModel(PreTrainedModel):\n",
    "        config_class = StaticEmbeddingConfig\n",
    "    \n",
    "        def __init__(self, config: StaticEmbeddingConfig):\n",
    "            super().__init__(config)\n",
    "            # ★ EmbeddingBag そのものでも OK ですが、\n",
    "            #   シーケンス長をそろえて attention_mask で平均を取る方が扱いやすいので nn.Embedding に変更\n",
    "            self.embedding = nn.Embedding(\n",
    "                num_embeddings=config.vocab_size,\n",
    "                embedding_dim=config.hidden_size,\n",
    "                padding_idx=config.pad_token_id,\n",
    "            )\n",
    "            self.post_init()  # transformers の重み初期化\n",
    "    \n",
    "        def forward(self, input_ids, attention_mask=None, **kwargs):\n",
    "            \"\"\"\n",
    "            - input_ids      : (batch, seq_len)\n",
    "            - attention_mask : (batch, seq_len) — 0 は padding\n",
    "            戻り値は Transformers 共通の BaseModelOutputWithPoolingAndCrossAttentions\n",
    "            \"\"\"\n",
    "            if attention_mask is None:\n",
    "                attention_mask = (input_ids != self.config.pad_token_id).int()\n",
    "    \n",
    "            token_embs = self.embedding(input_ids)                       # (B, L, H)\n",
    "            # マスク付き平均プール\n",
    "            masked_embs = token_embs * attention_mask.unsqueeze(-1)      # (B, L, H)\n",
    "            lengths = attention_mask.sum(dim=1, keepdim=True).clamp(min=1e-8)  # (B, 1)\n",
    "            sentence_emb = masked_embs.sum(dim=1) / lengths              # (B, H)\n",
    "    \n",
    "            return BaseModelOutputWithPoolingAndCrossAttentions(\n",
    "                last_hidden_state=token_embs,  # ここでは token レベルをそのまま\n",
    "                pooler_output=sentence_emb,    # 文ベクトル\n",
    "                attentions=None,\n",
    "                cross_attentions=None,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17442fc4-fc20-4821-8812-596ff97f4496",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_japanese_embedding:\n",
    "    \"\"\"\n",
    "    SentenceTransformer 版 (hotchpotch/static-embedding-japanese) から\n",
    "    StaticEmbeddingModel へ重みをコピーして保存するスクリプト\n",
    "    \"\"\"\n",
    "    \n",
    "    SRC = \"hotchpotch/static-embedding-japanese\"   # オリジナル\n",
    "    DST = \"./static-embedding-transformers\"        # 保存先\n",
    "    \n",
    "    # ① SentenceTransformer を読み込む\n",
    "    st_model = SentenceTransformer(SRC)\n",
    "    embedding_weight = st_model[0].embedding.weight.data   # nn.EmbeddingBag の重みを取得\n",
    "    \n",
    "    # ② Config → Model を作成\n",
    "    config = StaticEmbeddingConfig(\n",
    "        vocab_size=embedding_weight.size(0),\n",
    "        hidden_size=embedding_weight.size(1),\n",
    "        pad_token_id=0,           # トークナイザの <pad> が id=0\n",
    "    )\n",
    "    model = StaticEmbeddingModel(config)\n",
    "    \n",
    "    # ③ 重みコピー\n",
    "    with torch.no_grad():\n",
    "        model.embedding.weight.copy_(embedding_weight)\n",
    "    \n",
    "    # ④ save_pretrained で書き出し\n",
    "    model.save_pretrained(DST)\n",
    "    # st_model.tokenizer.save_pretrained(DST)   # tokenizer.json なども一緒に保存\n",
    "    \n",
    "    print(f\"✅ 変換完了 — 保存先: {DST}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd90d1ce-899e-4e4b-8c4b-ba6bc9545e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_japanese_embedding:\n",
    "    \n",
    "    MODEL_DIR = \"./static-embedding-transformers\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_MODEL)\n",
    "    model     = StaticEmbeddingModel.from_pretrained(MODEL_DIR)\n",
    "    \n",
    "    sentences = [\n",
    "        \"美味しいラーメン屋に行きたい\",\n",
    "        \"あそこは行きにくいけど、隠れた豚骨の名店だよ。スープが最高だし、麺の硬さも好み。\",\n",
    "    ]\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        sentences,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        add_special_tokens=False,   # 元モデルは special tokens なし\n",
    "    )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        vecs = outputs.pooler_output     # (batch, hidden_size)\n",
    "    \n",
    "    print(\"shape:\", vecs.shape)          # torch.Size([2, 1024])\n",
    "    similarity = torch.nn.functional.cosine_similarity(vecs[0], vecs[1], dim=0)\n",
    "    print(\"cosine:\", similarity.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5e501a8-78d3-431e-8849-cc7865683db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# どのエラーでリトライするかを定義\n",
    "# 429 (RateLimitError) や サーバー側のエラー(5xx) でリトライするのが一般的\n",
    "def should_retry(e: Exception) -> bool:\n",
    "    if isinstance(e, RateLimitError):\n",
    "        print(f\"RateLimitError発生。リトライします...: {e}\")\n",
    "        return True\n",
    "    # 5xx系のサーバーエラーでもリトライすることが多い\n",
    "    if isinstance(e, APIStatusError) and e.status_code >= 500:\n",
    "        print(f\"サーバーエラー( {e.status_code} )発生。リトライします...: {e}\")\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "@retry(\n",
    "    wait=wait_random_exponential(multiplier=1, max=70), # 1, 2, 4, 8秒...とランダムな時間を加えて待つ（最大70秒）\n",
    "    stop=stop_after_attempt(5), # 最大5回リトライする\n",
    "    retry=retry_if_exception(should_retry) # 上で定義した条件の例外が発生した場合にリトライ\n",
    ")\n",
    "async def openrouter_openai_complete(\n",
    "    prompt,\n",
    "    system_prompt=None,\n",
    "    history_messages=[],\n",
    "    keyword_extraction=False,\n",
    "    api_key: str = None,\n",
    "    **kwargs,\n",
    ") -> str:\n",
    "    # if api_key:\n",
    "    #     os.environ[\"OPENROUTER_API_KEY\"] = api_key\n",
    "\n",
    "    keyword_extraction = kwargs.pop(\"keyword_extraction\", None)\n",
    "    result = await openai_complete_if_cache(\n",
    "        LLM_MODEL,  # change accordingly\n",
    "        prompt,\n",
    "        system_prompt=system_prompt,\n",
    "        history_messages=history_messages,\n",
    "        base_url=\"https://openrouter.ai/api/v1\",\n",
    "        api_key=api_key,\n",
    "        **kwargs,\n",
    "    )\n",
    "    if keyword_extraction:  # TODO: use JSON API\n",
    "        return locate_json_string_body_from_string(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ea62583-fcbc-4e2a-9513-cc6bd4860f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/app\n",
      "MiniRAG_on_postgres.ipynb  minirag  setup.py\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627af843-ee1a-47ff-a0c4-bc4dcfe99ef8",
   "metadata": {},
   "source": [
    "### RAGシステムセットアップ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31b8ac6d-2322-4339-ae19-9b5c6193e521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------- ポスグレに接続しています -------------------------\n",
      "------------------------- ポスグレに接続しました！ -------------------------\n",
      "------------------------- テーブルの存在を確認・作成しています -------------------------\n",
      "------------------------- テーブルの準備が完了しました！ -------------------------\n",
      "------------------------- MiniRAGが初期化されました！ -------------------------\n"
     ]
    }
   ],
   "source": [
    "async def setup_rag_system():\n",
    "    \"\"\"RAGシステムを初期化し、準備ができたインスタンスを返す\"\"\"\n",
    "\n",
    "    # 1. データベース設定\n",
    "    db_config={\n",
    "        \"host\": \"postgres\",    # これは Docker のサービス名「postgres」で指定\n",
    "        \"port\": 5432,\n",
    "        \"user\": os.getenv(\"POSTGRES_USER\"),\n",
    "        \"password\": os.getenv(\"POSTGRES_PASSWORD\"),\n",
    "        \"database\": os.getenv(\"POSTGRES_DB\"),\n",
    "    }\n",
    "\n",
    "    # 2. PostgreSQLDBインスタンスを作成\n",
    "    db_postgre = PostgreSQLDB(config=db_config)\n",
    "\n",
    "    # 3. データベース接続を初期化\n",
    "    print(\"------------------------- ポスグレに接続しています -------------------------\")\n",
    "    await db_postgre.initdb()\n",
    "    print(\"------------------------- ポスグレに接続しました！ -------------------------\")\n",
    "\n",
    "    # 必要なテーブルが存在するかチェックし、なければ作成する\n",
    "    print(\"------------------------- テーブルの存在を確認・作成しています -------------------------\")\n",
    "    await db_postgre.check_tables()\n",
    "    print(\"------------------------- テーブルの準備が完了しました！ -------------------------\")\n",
    "\n",
    "    # 5. MiniRAGインスタンスを作成\n",
    "    os.environ[\"AGE_GRAPH_NAME\"] = \"my_minirag_graph\" # init_db.sh で指定したもの\n",
    "    rag = MiniRAG(\n",
    "        working_dir=WORKING_DIR,\n",
    "        # llm_model_func=hf_model_complete,\n",
    "        # llm_model_func=gemini_2_5_flash_complete,\n",
    "        llm_model_func=openrouter_openai_complete,\n",
    "        llm_model_max_token_size=200,\n",
    "        llm_model_name=LLM_MODEL,\n",
    "        embedding_func=EmbeddingFunc(\n",
    "            embedding_dim=EMBEDDING_DIM,\n",
    "            max_token_size=1000,\n",
    "            func=lambda texts: hf_embed(\n",
    "                texts,\n",
    "                tokenizer=tokenizer,\n",
    "                embed_model=model\n",
    "            ),\n",
    "        ),\n",
    "        kv_storage=\"PGKVStorage\",\n",
    "        vector_storage=\"PGVectorStorage\",\n",
    "        graph_storage=\"PGGraphStorage\",\n",
    "        doc_status_storage=\"PGDocStatusStorage\",\n",
    "        vector_db_storage_cls_kwargs={\n",
    "            \"cosine_better_than_threshold\": float(os.getenv(\"COSINE_THRESHOLD\"))\n",
    "        }\n",
    "    )\n",
    "    # データベースの情報を渡す\n",
    "    rag.set_storage_client(db_postgre)    \n",
    "    return rag\n",
    "\n",
    "# RAGシステムをセットアップ\n",
    "try:\n",
    "    rag = await setup_rag_system()\n",
    "    print(\"------------------------- MiniRAGが初期化されました！ -------------------------\")\n",
    "except Exception as e:\n",
    "    print(f\"RAGシステムのセットアップに失敗しました: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac44cacb-928d-4b21-acad-3ee53633b357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 約12分かかった\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# サンプルテキストデータ\n",
    "sample_texts = [\n",
    "    \"\"\"\n",
    "今日は素晴らしい一日でした。朝早く起きて、近所の公園を散歩しました。\n",
    "桜の花が満開で、とても美しかったです。午後は友人と映画を見に行きました。\n",
    "「君の名は。」という映画で、とても感動的でした。\n",
    "夜は家族と一緒に夕食を取り、楽しい時間を過ごしました。\n",
    "\"\"\",\n",
    "    \"\"\"\n",
    "昨日は仕事で大きなプロジェクトが完了しました。\n",
    "チーム全員で3ヶ月間取り組んできたAIシステムの開発が終わりました。\n",
    "機械学習モデルの精度が95%を超え、クライアントからも高い評価をいただきました。\n",
    "今夜はチームメンバーと祝賀会を開く予定です。\n",
    "\"\"\",\n",
    "    \"\"\"\n",
    "週末は料理に挑戦しました。初めてパスタを一から作ってみました。\n",
    "小麦粉から麺を作るのは思っていたより難しかったですが、\n",
    "最終的にはとても美味しいカルボナーラができました。\n",
    "次回はリゾットに挑戦してみたいと思います。\n",
    "\"\"\",\n",
    "    \"\"\"\n",
    "読書が趣味で、最近は村上春樹の「ノルウェイの森」を読んでいます。\n",
    "主人公の心情描写がとても繊細で、引き込まれます。\n",
    "また、技術書も読んでおり、「深層学習」について学んでいます。\n",
    "理論と実践のバランスが取れた良い本だと思います。\n",
    "\"\"\"\n",
    "]\n",
    "\n",
    "# データの挿入\n",
    "print(\"データを挿入中...\")\n",
    "\n",
    "async def insert_texts(rag_instance, texts):\n",
    "    for i, text in enumerate(texts):\n",
    "        print(f\"テキスト {i+1}/{len(texts)} を挿入中...\")\n",
    "        await rag_instance.ainsert(text.strip())\n",
    "\n",
    "    print(\"\\nすべてのデータが挿入されました！\")\n",
    "\n",
    "\n",
    "# イベントループが既に実行中の場合\n",
    "try:\n",
    "    await insert_texts(rag, sample_texts)\n",
    "except RuntimeError:\n",
    "    # 新しいループで実行\n",
    "    asyncio.run(insert_texts(rag, sample_texts))\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"処理時間: {elapsed_time:.4f}秒\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35031a37-d7b7-4a17-875e-a3f165cdb288",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 約3分かかった\n",
    "\n",
    "# # サンプルクエリ\n",
    "# queries = [\n",
    "#     \"映画について教えて\",\n",
    "#     \"仕事のプロジェクトはどうでしたか？\",\n",
    "#     \"料理で何を作りましたか？\",\n",
    "#     \"読んでいる本について教えて\",\n",
    "#     \"散歩について詳しく教えて\"\n",
    "# ]\n",
    "\n",
    "# # 各モードでクエリを実行。この3つがある\n",
    "# modes = [\"naive\", \"mini\", \"light\"]\n",
    "\n",
    "# for query in queries:\n",
    "#     print(f\"\\n{'='*50}\")\n",
    "#     print(f\"クエリ: {query}\")\n",
    "#     print(f\"{'='*50}\")\n",
    "\n",
    "#     for mode in modes:\n",
    "#         print(f\"\\n--- {mode.upper()}モード ---\")\n",
    "#         try:\n",
    "#             answer = rag.query(query, param=QueryParam(mode=mode))     # .replace(\"\\n\", \"\").replace(\"\\r\", \"\")\n",
    "#             print(f\"回答: {answer}\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"エラー: {e}\")\n",
    "\n",
    "\n",
    "async def run_queries(rag_instance, queries):\n",
    "    # 各モードでクエリを実行。この3つがある\n",
    "    modes = [\"naive\", \"mini\", \"light\"]\n",
    "\n",
    "    for query in queries:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"クエリ: {query}\")\n",
    "        print(f\"{'='*50}\")\n",
    "\n",
    "        for mode in modes:\n",
    "            print(f\"\\n--- {mode.upper()}モード ---\")\n",
    "            try:\n",
    "                # 非同期でクエリを実行\n",
    "                answer = await rag_instance.aquery(query, param=QueryParam(mode=mode))\n",
    "                print(f\"回答: {answer}\")\n",
    "            except Exception as e:\n",
    "                print(f\"エラー: {e}\")\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "sample_queries = [\n",
    "    \"映画について教えて\",\n",
    "    \"仕事のプロジェクトはどうでしたか？\",\n",
    "    \"料理で何を作りましたか？\",\n",
    "    \"読んでいる本について教えて\",\n",
    "    \"散歩について詳しく教えて\"\n",
    "]\n",
    "\n",
    "# イベントループが既に実行中の場合\n",
    "try:\n",
    "    await run_queries(rag, sample_queries)\n",
    "except RuntimeError:\n",
    "    # 新しいループで実行\n",
    "    asyncio.run(run_queries(rag, sample_queries))\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"処理時間: {elapsed_time:.4f}秒\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5621b47-01eb-468b-9e18-6d273ab147f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "1/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1daea0-3185-47ad-ba8d-116eb139386d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以下が色々出るのは仕様なので正常な動作。いっぱい出るのは並列処理しているらしい\n",
    "# 正常に動いていることを確認したいので、あえて出したままにしている。\n",
    "# ---------------------------------------------------------\n",
    "# ★デバッグ(postgres_impl.py): create_graph already exists\n",
    "\n",
    "\n",
    "\n",
    "# postgres16_age_pgvector_container  | 2025-07-22 04:43:01.467 UTC [1672] ERROR:  graph \"my_minirag_graph\" already exists\n",
    "# postgres16_age_pgvector_container  | 2025-07-22 04:43:01.467 UTC [1672] STATEMENT:  select create_graph('my_minirag_graph')\n",
    "\n",
    "\n",
    "\n",
    "# SELECT * FROM cypher('my_minirag_graph', $$\n",
    "#                      MATCH path = (start:Entity {node_id: \"xe980b1e69cab\"})-[*1..2]-(neighbor:Entity)\n",
    "#                      RETURN [n in nodes(path) | properties(n).node_id] AS path_nodes,\n",
    "#                             [r in relationships(path) | r] AS path_edges\n",
    "#                    $$) AS (path_nodes agtype, path_edges agtype)\n",
    "# None\n",
    "\n",
    "# 上記のクエリも合っているし Miniモード も正常に機能しているので問題ない。多分表示している箇所は PostgreSQLDB クラスの execute メソッドだと思う。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# INFO:minirag:Using the label default for PostgreSQL as identifier\n",
    "# INFO:minirag:Connected to PostgreSQL database at postgres:5432/my_database\n",
    "# ERROR:minirag:PostgreSQL database error: relation \"lightrag_doc_full\" does not exist\n",
    "# ERROR:minirag:Failed to check table LIGHTRAG_DOC_FULL in PostgreSQL database\n",
    "# ERROR:minirag:PostgreSQL database error: relation \"lightrag_doc_full\" does not exist\n",
    "# INFO:minirag:Created table LIGHTRAG_DOC_FULL in PostgreSQL database\n",
    "# ERROR:minirag:PostgreSQL database error: relation \"lightrag_doc_chunks\" does not exist\n",
    "# ERROR:minirag:Failed to check table LIGHTRAG_DOC_CHUNKS in PostgreSQL database\n",
    "# ERROR:minirag:PostgreSQL database error: relation \"lightrag_doc_chunks\" does not exist\n",
    "# INFO:minirag:Created table LIGHTRAG_DOC_CHUNKS in PostgreSQL database\n",
    "# ERROR:minirag:PostgreSQL database error: relation \"lightrag_vdb_entity\" does not exist\n",
    "# ERROR:minirag:Failed to check table LIGHTRAG_VDB_ENTITY in PostgreSQL database\n",
    "# ERROR:minirag:PostgreSQL database error: relation \"lightrag_vdb_entity\" does not exist\n",
    "# INFO:minirag:Created table LIGHTRAG_VDB_ENTITY in PostgreSQL database\n",
    "# ERROR:minirag:PostgreSQL database error: relation \"lightrag_vdb_relation\" does not exist\n",
    "# ERROR:minirag:Failed to check table LIGHTRAG_VDB_RELATION in PostgreSQL database\n",
    "# ERROR:minirag:PostgreSQL database error: relation \"lightrag_vdb_relation\" does not exist\n",
    "# INFO:minirag:Created table LIGHTRAG_VDB_RELATION in PostgreSQL database\n",
    "# ERROR:minirag:PostgreSQL database error: relation \"lightrag_llm_cache\" does not exist\n",
    "# ERROR:minirag:Failed to check table LIGHTRAG_LLM_CACHE in PostgreSQL database\n",
    "# ERROR:minirag:PostgreSQL database error: relation \"lightrag_llm_cache\" does not exist\n",
    "# INFO:minirag:Created table LIGHTRAG_LLM_CACHE in PostgreSQL database\n",
    "# ERROR:minirag:PostgreSQL database error: relation \"lightrag_doc_status\" does not exist\n",
    "# ERROR:minirag:Failed to check table LIGHTRAG_DOC_STATUS in PostgreSQL database\n",
    "# ERROR:minirag:PostgreSQL database error: relation \"lightrag_doc_status\" does not exist\n",
    "# INFO:minirag:Created table LIGHTRAG_DOC_STATUS in PostgreSQL database\n",
    "# INFO:minirag:Finished checking all tables in PostgreSQL database\n",
    "# INFO:minirag:Logger initialized for working directory: /tmp/minirag_demo\n",
    "\n",
    "# 上記の表示されている ERROR ログは、一見すると問題のように見えますが、実際には 「テーブルが存在しないことを検知した」 という正常な動作の一部です。check_tables メソッドは、このエラーを意図的に発生させて、テーブルが存在しない場合にのみ作成処理を行うように設計されています。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
